{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "cSpBHxxh7_fB",
        "ogeKlervn8_i",
        "udxm1Svk2eCE",
        "a6BOVU_-2TI-",
        "ih1oRiN86yts",
        "RSUJtGFr8ivo",
        "lSNFTpBmyaYa",
        "aiWrmeCa47lm",
        "jmVqsWwaB7T5",
        "4exg-_43B_Hp",
        "4YzANdndCC6n",
        "9rdZtMNAApMS",
        "56OLjGD8Aywx",
        "HsLC8P3mG79V",
        "7T9QO-0hMzLn",
        "AU8bkN60o3sH",
        "86OcLwOVx4M9",
        "2zrauGio67nc",
        "FKZc4rL87QBf",
        "Vl_wRjF47TN5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4975867e17a74a03bde9f485711e095d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c35fb6bc20b46e0b78fb6cb04906d1f",
              "IPY_MODEL_d65691bc0bb143f39cb840392ffca91c",
              "IPY_MODEL_20192817950a403a85485d7104129094"
            ],
            "layout": "IPY_MODEL_5679189b041348c99729546934dfffc5"
          }
        },
        "2c35fb6bc20b46e0b78fb6cb04906d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55af656731194514b3ef5e6554971467",
            "placeholder": "​",
            "style": "IPY_MODEL_2a21d7d8e1734704a431fa4a2a3e27f8",
            "value": "config.json: 100%"
          }
        },
        "d65691bc0bb143f39cb840392ffca91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8022b31345d74111a5b98fb7ac3f78d6",
            "max": 609,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b11a68a601d41b8b1e660a89813c913",
            "value": 609
          }
        },
        "20192817950a403a85485d7104129094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_428c651b7c0b40bcb5e93969fbe53e41",
            "placeholder": "​",
            "style": "IPY_MODEL_3217f796f614412389fadfa7de91f60e",
            "value": " 609/609 [00:00&lt;00:00, 49.9kB/s]"
          }
        },
        "5679189b041348c99729546934dfffc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55af656731194514b3ef5e6554971467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a21d7d8e1734704a431fa4a2a3e27f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8022b31345d74111a5b98fb7ac3f78d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b11a68a601d41b8b1e660a89813c913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "428c651b7c0b40bcb5e93969fbe53e41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3217f796f614412389fadfa7de91f60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fa82a9c0bf547c19e8d3a330473d405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_585788494678407f9e34eeabb8e965a5",
              "IPY_MODEL_7e70d503e29143c7911d02a9a6d1391c",
              "IPY_MODEL_fb158ecadb4b46a48c360f222cdc7154"
            ],
            "layout": "IPY_MODEL_6f585b84123840b6b8bbeb901aa2981e"
          }
        },
        "585788494678407f9e34eeabb8e965a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a8edeb878514b589e1c1f62084ca5d8",
            "placeholder": "​",
            "style": "IPY_MODEL_a85612868c994b6682382ae2b9d044c6",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "7e70d503e29143c7911d02a9a6d1391c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75447139983b4c4d81a57cdf2469dd5b",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18d27a9063a1456085ee2654ecfc5cee",
            "value": 26788
          }
        },
        "fb158ecadb4b46a48c360f222cdc7154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11071535c3e04c50be73fd85b904f55d",
            "placeholder": "​",
            "style": "IPY_MODEL_42f9402268394ff0a33e0c72a884604f",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 2.31MB/s]"
          }
        },
        "6f585b84123840b6b8bbeb901aa2981e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a8edeb878514b589e1c1f62084ca5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85612868c994b6682382ae2b9d044c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75447139983b4c4d81a57cdf2469dd5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d27a9063a1456085ee2654ecfc5cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11071535c3e04c50be73fd85b904f55d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42f9402268394ff0a33e0c72a884604f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e867bec43b5845d4862fb9a0f101a598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b659e2f6e3fb45aaadb15b025742d2c1",
              "IPY_MODEL_40608406e5d54b989f7dd0a2f3d1e684",
              "IPY_MODEL_acb535c38fc44463aec89185c073a136"
            ],
            "layout": "IPY_MODEL_57daec5e269145dfa1650f1de0e58c7c"
          }
        },
        "b659e2f6e3fb45aaadb15b025742d2c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4339e56613d64df9a43761b6b643e624",
            "placeholder": "​",
            "style": "IPY_MODEL_12da0e702c6f47ea98f1d6a253ad3cbb",
            "value": "Downloading shards: 100%"
          }
        },
        "40608406e5d54b989f7dd0a2f3d1e684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcc498b66e714b84bdfece907a6ae18b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_439f44aafbf04f5b9b5dd65eacff0a68",
            "value": 2
          }
        },
        "acb535c38fc44463aec89185c073a136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eb654e853a44e1f97ccafe38ac3f17f",
            "placeholder": "​",
            "style": "IPY_MODEL_a0fbd811b3b64a85af0ca81a77d8e113",
            "value": " 2/2 [01:48&lt;00:00, 48.63s/it]"
          }
        },
        "57daec5e269145dfa1650f1de0e58c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4339e56613d64df9a43761b6b643e624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12da0e702c6f47ea98f1d6a253ad3cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcc498b66e714b84bdfece907a6ae18b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "439f44aafbf04f5b9b5dd65eacff0a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1eb654e853a44e1f97ccafe38ac3f17f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0fbd811b3b64a85af0ca81a77d8e113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69e73198569746eea2e6927777196a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88c2fa2ee00a4d049cd1788ffcde40c3",
              "IPY_MODEL_dd3da97022f44449b5474c88257abaf1",
              "IPY_MODEL_b2b0b1fb316f49a4b30248d2a38f6fe6"
            ],
            "layout": "IPY_MODEL_0d9d38ef924f4dc49775781f133db16c"
          }
        },
        "88c2fa2ee00a4d049cd1788ffcde40c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0493d7cc9a2f4101870bb13bf465eaaf",
            "placeholder": "​",
            "style": "IPY_MODEL_210eabe13631403b8ea135c247af9cd4",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "dd3da97022f44449b5474c88257abaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d27f1db84e49d6b4fc5837328d6085",
            "max": 9976578928,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e38262147c4344d38f8e0a979526ac2c",
            "value": 9976578928
          }
        },
        "b2b0b1fb316f49a4b30248d2a38f6fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ead36c4c903c4fcb83d6a43f852b0646",
            "placeholder": "​",
            "style": "IPY_MODEL_df42e5f45f05443baf063c5f455bfd87",
            "value": " 9.98G/9.98G [01:24&lt;00:00, 80.3MB/s]"
          }
        },
        "0d9d38ef924f4dc49775781f133db16c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0493d7cc9a2f4101870bb13bf465eaaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "210eabe13631403b8ea135c247af9cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d27f1db84e49d6b4fc5837328d6085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38262147c4344d38f8e0a979526ac2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ead36c4c903c4fcb83d6a43f852b0646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df42e5f45f05443baf063c5f455bfd87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e419e453f4fa4acf845ee63c12c0411b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b24eea17bf1c4beaae7ae1f3c6112342",
              "IPY_MODEL_7789de6fc5bc442c8f15d2f5e64b91d2",
              "IPY_MODEL_ed14d0275a374b2e9c55a72d2662f0a6"
            ],
            "layout": "IPY_MODEL_35f9202fcd1241b0b21041b7cca74cb3"
          }
        },
        "b24eea17bf1c4beaae7ae1f3c6112342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6df59d2a0c3487ab74d7e68691e25ea",
            "placeholder": "​",
            "style": "IPY_MODEL_5385e9f280934b52b01c616e434e9b3c",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "7789de6fc5bc442c8f15d2f5e64b91d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eca48c7212314753b4c584319b7e2794",
            "max": 3500297344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_152a23cb4d104f9f867fc6026faf3ede",
            "value": 3500297344
          }
        },
        "ed14d0275a374b2e9c55a72d2662f0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb84b72907994f2785f7177d8d9017ad",
            "placeholder": "​",
            "style": "IPY_MODEL_09841d00a40b493a8637cb1eede1ab27",
            "value": " 3.50G/3.50G [00:23&lt;00:00, 148MB/s]"
          }
        },
        "35f9202fcd1241b0b21041b7cca74cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6df59d2a0c3487ab74d7e68691e25ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5385e9f280934b52b01c616e434e9b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eca48c7212314753b4c584319b7e2794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152a23cb4d104f9f867fc6026faf3ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb84b72907994f2785f7177d8d9017ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09841d00a40b493a8637cb1eede1ab27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b13af03087604e2d8cfd3c939e742465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd53d018adf64f02ae024877a1a4cfb6",
              "IPY_MODEL_800a28c15c12429eb95029a6f4f0027b",
              "IPY_MODEL_c267443c192c4ca8af25ff28dd41f3b4"
            ],
            "layout": "IPY_MODEL_93a3b1e3532d49949b3c2d2514146f65"
          }
        },
        "dd53d018adf64f02ae024877a1a4cfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_134480701bb64a95926d94a86e9099c9",
            "placeholder": "​",
            "style": "IPY_MODEL_873d5c56e9c94c329eb88925802a2245",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "800a28c15c12429eb95029a6f4f0027b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b5789303e134920ab00ef5f19c2b3ca",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cbe4077be2e408ab8cc27d79f5afafc",
            "value": 2
          }
        },
        "c267443c192c4ca8af25ff28dd41f3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1fff5761811452e9c5a95bb93ecac95",
            "placeholder": "​",
            "style": "IPY_MODEL_52245fa9021a4a2ebafcc923df1595e1",
            "value": " 2/2 [00:04&lt;00:00,  2.35s/it]"
          }
        },
        "93a3b1e3532d49949b3c2d2514146f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134480701bb64a95926d94a86e9099c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "873d5c56e9c94c329eb88925802a2245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b5789303e134920ab00ef5f19c2b3ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cbe4077be2e408ab8cc27d79f5afafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1fff5761811452e9c5a95bb93ecac95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52245fa9021a4a2ebafcc923df1595e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c478e7c73b7444ab98315683cb44fecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81f849257ad443c1961ccaa7e894c09e",
              "IPY_MODEL_61c1c09211d848978c11db7c9f2115c4",
              "IPY_MODEL_a230e6e061084bc2bb0df82dd6d49fda"
            ],
            "layout": "IPY_MODEL_b7e3ffd72d0b4d65be026fbc3a5657c7"
          }
        },
        "81f849257ad443c1961ccaa7e894c09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58f049726f83454d88d52a051880651c",
            "placeholder": "​",
            "style": "IPY_MODEL_a367155e8f484dcd81cc752a82d9b836",
            "value": "generation_config.json: 100%"
          }
        },
        "61c1c09211d848978c11db7c9f2115c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98941e82fc3d4a04b86d7929731ca1ec",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ce2ae41bf724879a95e3e35741cf682",
            "value": 188
          }
        },
        "a230e6e061084bc2bb0df82dd6d49fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6de9d5a8644a41f58db1a4cd80c38599",
            "placeholder": "​",
            "style": "IPY_MODEL_6e27df0b81864747a03ad861b7c72ec1",
            "value": " 188/188 [00:00&lt;00:00, 16.9kB/s]"
          }
        },
        "b7e3ffd72d0b4d65be026fbc3a5657c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58f049726f83454d88d52a051880651c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a367155e8f484dcd81cc752a82d9b836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98941e82fc3d4a04b86d7929731ca1ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce2ae41bf724879a95e3e35741cf682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6de9d5a8644a41f58db1a4cd80c38599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e27df0b81864747a03ad861b7c72ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f9a3f7006e54a7b9b03a84cd97a5d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6126a8be6cd84c2da1b05899b7f581e4",
              "IPY_MODEL_5f0eac648125443f9e8b0bfbae185b4c",
              "IPY_MODEL_91c5358e3ac44a8e81cd0f88726d23d7"
            ],
            "layout": "IPY_MODEL_ef99672c926d4fb28f7e93c7fbc4e6fe"
          }
        },
        "6126a8be6cd84c2da1b05899b7f581e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84f7bda550e43efb9c5c9ca671f7d70",
            "placeholder": "​",
            "style": "IPY_MODEL_ddbaff305e794dfa872436968b524df0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "5f0eac648125443f9e8b0bfbae185b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fcd5ed0976848fab1457fe00ecb05f0",
            "max": 776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_276d9952e424408db6cae6bf938382e3",
            "value": 776
          }
        },
        "91c5358e3ac44a8e81cd0f88726d23d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b2d3dcb50614523b41e0efd16b59201",
            "placeholder": "​",
            "style": "IPY_MODEL_f9afa5a836994c84a5c185f8745de271",
            "value": " 776/776 [00:00&lt;00:00, 76.5kB/s]"
          }
        },
        "ef99672c926d4fb28f7e93c7fbc4e6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84f7bda550e43efb9c5c9ca671f7d70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddbaff305e794dfa872436968b524df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fcd5ed0976848fab1457fe00ecb05f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "276d9952e424408db6cae6bf938382e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b2d3dcb50614523b41e0efd16b59201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9afa5a836994c84a5c185f8745de271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9da98701994142b7ddc66e44c2e295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ced3c1591ff4b53b23d8007df9b386e",
              "IPY_MODEL_1dd2e9bb4a7547f68a759a59b0d569bf",
              "IPY_MODEL_32cf093d103a4164802849ddebd41c68"
            ],
            "layout": "IPY_MODEL_ab36dfda552a45299c3b9c44a0b5bb29"
          }
        },
        "1ced3c1591ff4b53b23d8007df9b386e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac1a9992fe5145af806d9b4987f299d1",
            "placeholder": "​",
            "style": "IPY_MODEL_daf754d987b44f1bb9b5309af168a4b4",
            "value": "tokenizer.model: 100%"
          }
        },
        "1dd2e9bb4a7547f68a759a59b0d569bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a57460ebb724e3b9e48177bd52e70d6",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4da04151c5454641906172c3403a1162",
            "value": 499723
          }
        },
        "32cf093d103a4164802849ddebd41c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c03fdcb49baf47938deb827b72199b7b",
            "placeholder": "​",
            "style": "IPY_MODEL_8761f860c06f4fbfb6df594bdca0a080",
            "value": " 500k/500k [00:00&lt;00:00, 37.2MB/s]"
          }
        },
        "ab36dfda552a45299c3b9c44a0b5bb29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac1a9992fe5145af806d9b4987f299d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daf754d987b44f1bb9b5309af168a4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a57460ebb724e3b9e48177bd52e70d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da04151c5454641906172c3403a1162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c03fdcb49baf47938deb827b72199b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8761f860c06f4fbfb6df594bdca0a080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b07997d3aeed4a7fa2408c1ebeb9db63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56de422470174dffbf8cad2ab931f001",
              "IPY_MODEL_7d7bbfb29bf14d0ebd52751f54111e1f",
              "IPY_MODEL_e52736e700e346ee8e981cd725eeb843"
            ],
            "layout": "IPY_MODEL_982c7136599849a2b603e28a3db89523"
          }
        },
        "56de422470174dffbf8cad2ab931f001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffd37ff1156f43bf8ef4c08660a8d04f",
            "placeholder": "​",
            "style": "IPY_MODEL_7b6c4388db434eb4b59b6f34edd26235",
            "value": "tokenizer.json: 100%"
          }
        },
        "7d7bbfb29bf14d0ebd52751f54111e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f3ccb5aeb4b41b19f93865992bfd556",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69d9da75f32e4a38b75717068617987b",
            "value": 1842767
          }
        },
        "e52736e700e346ee8e981cd725eeb843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a57584aab00c4ab28af6882a96e658ba",
            "placeholder": "​",
            "style": "IPY_MODEL_3b165f1c9b754923ab9b31a9ae675645",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 32.3MB/s]"
          }
        },
        "982c7136599849a2b603e28a3db89523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd37ff1156f43bf8ef4c08660a8d04f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6c4388db434eb4b59b6f34edd26235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f3ccb5aeb4b41b19f93865992bfd556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d9da75f32e4a38b75717068617987b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a57584aab00c4ab28af6882a96e658ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b165f1c9b754923ab9b31a9ae675645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efd1af09bf6c40af948d90ca02357c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4da4516568647e996fab9eb98594e76",
              "IPY_MODEL_112526326b4a41ec80d79fb4f5bf1e3c",
              "IPY_MODEL_e44e2992673745c6983afa4f38d5d6ee"
            ],
            "layout": "IPY_MODEL_a1e7cc3539c34083b8f63d145d67ffd9"
          }
        },
        "a4da4516568647e996fab9eb98594e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae895295ef64b9da63f5e785f00de4f",
            "placeholder": "​",
            "style": "IPY_MODEL_164ec6175d8e4fbaab4cf680401d51b2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "112526326b4a41ec80d79fb4f5bf1e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e4e25874aed4f8289ce9310ff875546",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47b081bcee11420ba39991823a0022e4",
            "value": 414
          }
        },
        "e44e2992673745c6983afa4f38d5d6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbb9bb8802ea4350b6b6588637dc9747",
            "placeholder": "​",
            "style": "IPY_MODEL_217706d339c2409e92e9cb3602f8f46b",
            "value": " 414/414 [00:00&lt;00:00, 36.6kB/s]"
          }
        },
        "a1e7cc3539c34083b8f63d145d67ffd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae895295ef64b9da63f5e785f00de4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "164ec6175d8e4fbaab4cf680401d51b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e4e25874aed4f8289ce9310ff875546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b081bcee11420ba39991823a0022e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbb9bb8802ea4350b6b6588637dc9747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217706d339c2409e92e9cb3602f8f46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42dbd081adce424693c4ad1176c26b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77fbb932a1504e4884c8d5c99abc8b7f",
              "IPY_MODEL_29ecabceec5047a6a22e3a982f7884e5",
              "IPY_MODEL_2fd9eb896c294ef1a6ec0f1a037b4531"
            ],
            "layout": "IPY_MODEL_56660c4c0696404b8a620c3d6b4ddd33"
          }
        },
        "77fbb932a1504e4884c8d5c99abc8b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831939e5244b4fe69a721b1ceb36a9de",
            "placeholder": "​",
            "style": "IPY_MODEL_c9981202e9844afa9e836bdeb935e16c",
            "value": "Downloading builder script: 100%"
          }
        },
        "29ecabceec5047a6a22e3a982f7884e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd4400e748954121b4134877ae7713b4",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a113570540624a31ae4848fe557471f1",
            "value": 6270
          }
        },
        "2fd9eb896c294ef1a6ec0f1a037b4531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f9eedfae4144239c69092cccac3898",
            "placeholder": "​",
            "style": "IPY_MODEL_b329164620c441798e446c905b72e4db",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 519kB/s]"
          }
        },
        "56660c4c0696404b8a620c3d6b4ddd33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "831939e5244b4fe69a721b1ceb36a9de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9981202e9844afa9e836bdeb935e16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd4400e748954121b4134877ae7713b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a113570540624a31ae4848fe557471f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3f9eedfae4144239c69092cccac3898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b329164620c441798e446c905b72e4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa4485acd6354c919c732b772df11cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d95bf35c8b974eb589235bb21517ebde",
              "IPY_MODEL_15d97605b48044588f7261225e7172e9",
              "IPY_MODEL_fcd03d9e76854c6ba924dea0d2022587"
            ],
            "layout": "IPY_MODEL_fc6031c3aa584388bccfe7044cf605e0"
          }
        },
        "d95bf35c8b974eb589235bb21517ebde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec3f0892c3df4e91a41750ee60a71c61",
            "placeholder": "​",
            "style": "IPY_MODEL_5a8bba555bb64681be8cbb4390fc4801",
            "value": "modules.json: 100%"
          }
        },
        "15d97605b48044588f7261225e7172e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af55b1b1bc1543ac869e4a49b34e506d",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f9e31d0a0b141fc8697e9ad3594c0b6",
            "value": 349
          }
        },
        "fcd03d9e76854c6ba924dea0d2022587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f75342ee7474d189a0d5d38ae3b8305",
            "placeholder": "​",
            "style": "IPY_MODEL_5f20d56150614abcaa62769361ac55b1",
            "value": " 349/349 [00:00&lt;00:00, 30.4kB/s]"
          }
        },
        "fc6031c3aa584388bccfe7044cf605e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec3f0892c3df4e91a41750ee60a71c61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8bba555bb64681be8cbb4390fc4801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af55b1b1bc1543ac869e4a49b34e506d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f9e31d0a0b141fc8697e9ad3594c0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f75342ee7474d189a0d5d38ae3b8305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f20d56150614abcaa62769361ac55b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e90fba342eaf4740aba2f1543bf9819d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ad29fe283924b5fa81562d01ce2cc4f",
              "IPY_MODEL_b119b221f03e424a836daeba33627eee",
              "IPY_MODEL_48e0624a7f2f4a0a933a895b600dbf81"
            ],
            "layout": "IPY_MODEL_21e1fb5392e346dd8c6dce4e9e0ffef6"
          }
        },
        "8ad29fe283924b5fa81562d01ce2cc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f6d3b12f57c40138ef2e36a6380b3a6",
            "placeholder": "​",
            "style": "IPY_MODEL_dd4493244aa04fb5bae7c4a546ea48d0",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "b119b221f03e424a836daeba33627eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a296dad9a34e4497b0d13d1243745b58",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1be02d48484546bd9a334a14ca58449a",
            "value": 116
          }
        },
        "48e0624a7f2f4a0a933a895b600dbf81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ab2f4682f144861af923dd850193ed4",
            "placeholder": "​",
            "style": "IPY_MODEL_4e9659097f2a42e8ba700589dd9cdadd",
            "value": " 116/116 [00:00&lt;00:00, 10.7kB/s]"
          }
        },
        "21e1fb5392e346dd8c6dce4e9e0ffef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6d3b12f57c40138ef2e36a6380b3a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4493244aa04fb5bae7c4a546ea48d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a296dad9a34e4497b0d13d1243745b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be02d48484546bd9a334a14ca58449a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ab2f4682f144861af923dd850193ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e9659097f2a42e8ba700589dd9cdadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fef32b949ea4c32bd81a0d51580bbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f9c76c20d0840908cddd2f910cd2dc7",
              "IPY_MODEL_3f3aebb4306a4cbba56c32a754a8fcc3",
              "IPY_MODEL_3969b496c25b4edcae9b5d0ebde0f91c"
            ],
            "layout": "IPY_MODEL_60b6b0da3c4140e0b2b5974d5584e42f"
          }
        },
        "0f9c76c20d0840908cddd2f910cd2dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd43b4ae4423463581e95822ab47c4f6",
            "placeholder": "​",
            "style": "IPY_MODEL_0376cf246d4d4692ac5e1850db436c75",
            "value": "README.md: 100%"
          }
        },
        "3f3aebb4306a4cbba56c32a754a8fcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ef7b053ed24fb58e0c475e24301462",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2427c848c7b845dea2ec263ad22c801d",
            "value": 10659
          }
        },
        "3969b496c25b4edcae9b5d0ebde0f91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63fa1462ee714baabc61b1085ac8596e",
            "placeholder": "​",
            "style": "IPY_MODEL_e71cd88e8e224ce08b99a05978ac0cf9",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 928kB/s]"
          }
        },
        "60b6b0da3c4140e0b2b5974d5584e42f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd43b4ae4423463581e95822ab47c4f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0376cf246d4d4692ac5e1850db436c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ef7b053ed24fb58e0c475e24301462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2427c848c7b845dea2ec263ad22c801d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63fa1462ee714baabc61b1085ac8596e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e71cd88e8e224ce08b99a05978ac0cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38b3b0dd603f4be1b5e271f25d44f49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c30c3557d3974af08e5e6ae15562e817",
              "IPY_MODEL_22cfcafaef65496eab1eef9ab82ca301",
              "IPY_MODEL_02309359b597475eb11c2d732b8e468f"
            ],
            "layout": "IPY_MODEL_7a102e77bfc54d94b098a9a5a4df7a20"
          }
        },
        "c30c3557d3974af08e5e6ae15562e817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316e9bf4267547079076e002306a612f",
            "placeholder": "​",
            "style": "IPY_MODEL_3241d6f6d2c4450c9a91d7d2c7287ec7",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "22cfcafaef65496eab1eef9ab82ca301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b20cb36ead384c8ea87387b94378e890",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_923b8c7bdb984631b3166ef886f67fdf",
            "value": 53
          }
        },
        "02309359b597475eb11c2d732b8e468f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f26ca1291fd4d2aa4653cefb3fba27a",
            "placeholder": "​",
            "style": "IPY_MODEL_5fc45663a7e440ac87809bedd97c5433",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.75kB/s]"
          }
        },
        "7a102e77bfc54d94b098a9a5a4df7a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "316e9bf4267547079076e002306a612f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3241d6f6d2c4450c9a91d7d2c7287ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b20cb36ead384c8ea87387b94378e890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "923b8c7bdb984631b3166ef886f67fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f26ca1291fd4d2aa4653cefb3fba27a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fc45663a7e440ac87809bedd97c5433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d14b977834894816bd5720062f0be83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a26dc35ae6df4584bc50a37399b35735",
              "IPY_MODEL_310eef4214894e78bc8a088729097c76",
              "IPY_MODEL_721a0e697e1943f689606d25841ed241"
            ],
            "layout": "IPY_MODEL_15481698b6d44400a767b00d1749d067"
          }
        },
        "a26dc35ae6df4584bc50a37399b35735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cf2b763a4de4652bf0fc15ec5c567bd",
            "placeholder": "​",
            "style": "IPY_MODEL_84df9c07222546bf945f8d1293e5ad9d",
            "value": "config.json: 100%"
          }
        },
        "310eef4214894e78bc8a088729097c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5cc399af5184800a4779567b1eb256c",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a58a9008d8d54221a00bdd3ebca64308",
            "value": 612
          }
        },
        "721a0e697e1943f689606d25841ed241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dab3350ea8d478090f7447b9f870dae",
            "placeholder": "​",
            "style": "IPY_MODEL_47ab2f787a2a4ecba929181c27626d6b",
            "value": " 612/612 [00:00&lt;00:00, 54.0kB/s]"
          }
        },
        "15481698b6d44400a767b00d1749d067": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf2b763a4de4652bf0fc15ec5c567bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84df9c07222546bf945f8d1293e5ad9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5cc399af5184800a4779567b1eb256c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58a9008d8d54221a00bdd3ebca64308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dab3350ea8d478090f7447b9f870dae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ab2f787a2a4ecba929181c27626d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "114f8636dbf84abc8a21a73e911b9e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_224cfaf140414408b3edbc71af671099",
              "IPY_MODEL_c4e025464b7b4d42b3bd75873c12d96f",
              "IPY_MODEL_e2628d50dee049fdb23e7249c940d392"
            ],
            "layout": "IPY_MODEL_e0e364699c2249ffbc4760be28c2e9b7"
          }
        },
        "224cfaf140414408b3edbc71af671099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec8fe9cc012428c83896697278770a8",
            "placeholder": "​",
            "style": "IPY_MODEL_36a71204973b4c75b38ad353a3b62955",
            "value": "model.safetensors: 100%"
          }
        },
        "c4e025464b7b4d42b3bd75873c12d96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a5d956eb5541888b34492a2cc91b71",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e8a841036cd42888c7b0a7bc334233d",
            "value": 90868376
          }
        },
        "e2628d50dee049fdb23e7249c940d392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c53401dd4ce6479ab66b05839fc22170",
            "placeholder": "​",
            "style": "IPY_MODEL_6ff116f04f5547178415f24ca3546bcb",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 154MB/s]"
          }
        },
        "e0e364699c2249ffbc4760be28c2e9b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec8fe9cc012428c83896697278770a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36a71204973b4c75b38ad353a3b62955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66a5d956eb5541888b34492a2cc91b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e8a841036cd42888c7b0a7bc334233d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c53401dd4ce6479ab66b05839fc22170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ff116f04f5547178415f24ca3546bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caba4de8a5a14d93bdcb10855ca104e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5975a24fc1a04ff5aa83ed329d8ec572",
              "IPY_MODEL_c0175388c3184b5d9b677b8b1d07ef8e",
              "IPY_MODEL_26d9f51df57a465f88427eb89f10002d"
            ],
            "layout": "IPY_MODEL_bb50c10d7c9b41d8bd275af57266de8e"
          }
        },
        "5975a24fc1a04ff5aa83ed329d8ec572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb93985c963b4d08919516e499dba7c8",
            "placeholder": "​",
            "style": "IPY_MODEL_965bf765b4c04b08a6d41d53c113bdf1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c0175388c3184b5d9b677b8b1d07ef8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc2e27c2b7844153b82d7f35b0e87363",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b8381b81366487a9ac3be770416ae75",
            "value": 350
          }
        },
        "26d9f51df57a465f88427eb89f10002d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfde5161b39d4eaa9247f8e472b63c0d",
            "placeholder": "​",
            "style": "IPY_MODEL_8ea853a37b5948b4ab5c9cf8965ee00c",
            "value": " 350/350 [00:00&lt;00:00, 31.0kB/s]"
          }
        },
        "bb50c10d7c9b41d8bd275af57266de8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb93985c963b4d08919516e499dba7c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965bf765b4c04b08a6d41d53c113bdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc2e27c2b7844153b82d7f35b0e87363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b8381b81366487a9ac3be770416ae75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfde5161b39d4eaa9247f8e472b63c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ea853a37b5948b4ab5c9cf8965ee00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dea1f65ec264848a670c3ec3c2441bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1099be9264b54963825ee8df8906fb37",
              "IPY_MODEL_e7c6413f3e264573872b24eac2ded35f",
              "IPY_MODEL_4cebfbf3039140589d8b6c68940e7d6a"
            ],
            "layout": "IPY_MODEL_8330ad32f8324a9dbfaf9cdd61e41edc"
          }
        },
        "1099be9264b54963825ee8df8906fb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895bbdf3ebee49ef80b449eb53d047ae",
            "placeholder": "​",
            "style": "IPY_MODEL_23474725f5ab449a93c13f45940d2b85",
            "value": "vocab.txt: 100%"
          }
        },
        "e7c6413f3e264573872b24eac2ded35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff54fd5941646b282e9997add7576fc",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7391d2c144143019561365ed4177407",
            "value": 231508
          }
        },
        "4cebfbf3039140589d8b6c68940e7d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9edbcdd6a434c95830c039d7a0cc11e",
            "placeholder": "​",
            "style": "IPY_MODEL_a7eb2bb9f2df437284a37bfc3d9416fb",
            "value": " 232k/232k [00:00&lt;00:00, 6.92MB/s]"
          }
        },
        "8330ad32f8324a9dbfaf9cdd61e41edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "895bbdf3ebee49ef80b449eb53d047ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23474725f5ab449a93c13f45940d2b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ff54fd5941646b282e9997add7576fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7391d2c144143019561365ed4177407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9edbcdd6a434c95830c039d7a0cc11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7eb2bb9f2df437284a37bfc3d9416fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68771d16206941988c543348845455cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d4266334e3647759780153c3091676a",
              "IPY_MODEL_cf2c1180c63147d7ab40d5fe9573accd",
              "IPY_MODEL_f2c3239b52c044ce838c4a045443c485"
            ],
            "layout": "IPY_MODEL_a238bee01ea3461fb8d77ac20f1da406"
          }
        },
        "2d4266334e3647759780153c3091676a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fb5cb7c03d9445881eecbe4d870d049",
            "placeholder": "​",
            "style": "IPY_MODEL_d009c81f43264f64938fb747c41acd02",
            "value": "tokenizer.json: 100%"
          }
        },
        "cf2c1180c63147d7ab40d5fe9573accd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31db28fcc4674f229567488358553142",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78d8f9bd55384c66b38b603e9658496a",
            "value": 466247
          }
        },
        "f2c3239b52c044ce838c4a045443c485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20d832e73bb74b088774bc7209eb3306",
            "placeholder": "​",
            "style": "IPY_MODEL_5878388007184c07b06d6c89156cdd0d",
            "value": " 466k/466k [00:00&lt;00:00, 32.0MB/s]"
          }
        },
        "a238bee01ea3461fb8d77ac20f1da406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb5cb7c03d9445881eecbe4d870d049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d009c81f43264f64938fb747c41acd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31db28fcc4674f229567488358553142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d8f9bd55384c66b38b603e9658496a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20d832e73bb74b088774bc7209eb3306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5878388007184c07b06d6c89156cdd0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eda82ec6a7374cea8aac0fc1fdad40b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dec6367cd5b47c68086efe8e97a8338",
              "IPY_MODEL_362b0efe763742059115343e3b5af71e",
              "IPY_MODEL_6ed30834879e4e39b90e31271aec6f05"
            ],
            "layout": "IPY_MODEL_6bad7728ef2a4bf391f1e3615a299d45"
          }
        },
        "0dec6367cd5b47c68086efe8e97a8338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbb92ac85b13480d90dfde1ee95973f0",
            "placeholder": "​",
            "style": "IPY_MODEL_f9bc096b3dbf40caa53c6c6cd8b0872d",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "362b0efe763742059115343e3b5af71e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80a103b4e1aa49f3a26909532e26046c",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bade2c1ae0bb498aa700fd80120d5c43",
            "value": 112
          }
        },
        "6ed30834879e4e39b90e31271aec6f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b3cb0c984f404999c4499b0c92d688",
            "placeholder": "​",
            "style": "IPY_MODEL_a418c1e6601d4e078fe91c1d78c58047",
            "value": " 112/112 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "6bad7728ef2a4bf391f1e3615a299d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbb92ac85b13480d90dfde1ee95973f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9bc096b3dbf40caa53c6c6cd8b0872d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80a103b4e1aa49f3a26909532e26046c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bade2c1ae0bb498aa700fd80120d5c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5b3cb0c984f404999c4499b0c92d688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a418c1e6601d4e078fe91c1d78c58047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84e7ccd35ee14808bda6de851f5177c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ceead924a9ef4bd89549643881e5959a",
              "IPY_MODEL_7e28b22171d94a5fb186e3a7cc50bf7f",
              "IPY_MODEL_1ed6d606e18d4abab68606c16a8bfe17"
            ],
            "layout": "IPY_MODEL_5cc6364d9da548d594664e436ceb8f10"
          }
        },
        "ceead924a9ef4bd89549643881e5959a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04fc77f663194a81b2dfefd7efe2ae12",
            "placeholder": "​",
            "style": "IPY_MODEL_245742486b2647d08489f86f33709aca",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "7e28b22171d94a5fb186e3a7cc50bf7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcd3242b029841d19d851f9bf7ec7f58",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4eb74eff95044cb850e036911e1daf3",
            "value": 190
          }
        },
        "1ed6d606e18d4abab68606c16a8bfe17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54c02a6a3902495cb44456b6cd6d7434",
            "placeholder": "​",
            "style": "IPY_MODEL_afb59ec28288493f9b0c799d9c54ce26",
            "value": " 190/190 [00:00&lt;00:00, 16.3kB/s]"
          }
        },
        "5cc6364d9da548d594664e436ceb8f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04fc77f663194a81b2dfefd7efe2ae12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "245742486b2647d08489f86f33709aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcd3242b029841d19d851f9bf7ec7f58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4eb74eff95044cb850e036911e1daf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54c02a6a3902495cb44456b6cd6d7434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afb59ec28288493f9b0c799d9c54ce26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52247574206c4ec6bf58ff0711816d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2c9d43abbf54470b3ddca9eda200528",
              "IPY_MODEL_f62532f4eb334197aac1894fefa19301",
              "IPY_MODEL_761b65faaa484e1c88576ced33e3521d"
            ],
            "layout": "IPY_MODEL_4c523d0af93a4cc7ae5c4c7f742f1f9f"
          }
        },
        "a2c9d43abbf54470b3ddca9eda200528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73cd906f7b01430ebf23c6d30c9bee0e",
            "placeholder": "​",
            "style": "IPY_MODEL_6969b7e08fc04c6fa0c2d4608f9b0ed1",
            "value": "README.md: 100%"
          }
        },
        "f62532f4eb334197aac1894fefa19301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37baf2a797514150a6ccffaf01aea16f",
            "max": 7472,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d26a4de6d72f4a768ee0067f901183bd",
            "value": 7472
          }
        },
        "761b65faaa484e1c88576ced33e3521d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_346081f2c3064f7cb407b4a9a96822f6",
            "placeholder": "​",
            "style": "IPY_MODEL_16ede40e2c6b4b37b6ef68b1cbfe0128",
            "value": " 7.47k/7.47k [00:00&lt;00:00, 689kB/s]"
          }
        },
        "4c523d0af93a4cc7ae5c4c7f742f1f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cd906f7b01430ebf23c6d30c9bee0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6969b7e08fc04c6fa0c2d4608f9b0ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37baf2a797514150a6ccffaf01aea16f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26a4de6d72f4a768ee0067f901183bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "346081f2c3064f7cb407b4a9a96822f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ede40e2c6b4b37b6ef68b1cbfe0128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de650b541ecb42e49493d9a520fc103b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_445dd2514ec34e0aab64b2c189f95d9f",
              "IPY_MODEL_0292bd7196cd4f2fb2a39244ec26ce1a",
              "IPY_MODEL_3f54425fc8f141ea9abf2e5cb68b0cfc"
            ],
            "layout": "IPY_MODEL_b6204c6b5a374cfb97de640041555f0e"
          }
        },
        "445dd2514ec34e0aab64b2c189f95d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21459a23abf641ee9f0eabbc0bf151ff",
            "placeholder": "​",
            "style": "IPY_MODEL_ac27b08413ce4b9da28c244b788de4df",
            "value": "(…)-00000-of-00001-a09b74b3ef9c3b56.parquet: 100%"
          }
        },
        "0292bd7196cd4f2fb2a39244ec26ce1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0c97e29332e4416bb7b07301efbb02d",
            "max": 24246638,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5832878c00b34ec0b9cc02102f5f036c",
            "value": 24246638
          }
        },
        "3f54425fc8f141ea9abf2e5cb68b0cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70df0c04023d4af68eff2fc33d67a52a",
            "placeholder": "​",
            "style": "IPY_MODEL_2367f2be51984ee0a6fdb830d6cbead2",
            "value": " 24.2M/24.2M [00:00&lt;00:00, 56.1MB/s]"
          }
        },
        "b6204c6b5a374cfb97de640041555f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21459a23abf641ee9f0eabbc0bf151ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac27b08413ce4b9da28c244b788de4df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0c97e29332e4416bb7b07301efbb02d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5832878c00b34ec0b9cc02102f5f036c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70df0c04023d4af68eff2fc33d67a52a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2367f2be51984ee0a6fdb830d6cbead2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dc425eadcc242b7aa83f1a890069676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_093a850bebd9418b9652beaa0209b4e3",
              "IPY_MODEL_43c945e9ad154773bfe9ddd21d84cb69",
              "IPY_MODEL_d898aab25e1e4e3db4a788f0a3954a9a"
            ],
            "layout": "IPY_MODEL_e850e41203ed44a196c35f977e7fcd2e"
          }
        },
        "093a850bebd9418b9652beaa0209b4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd315c9799644c13b96fe43e6dea2241",
            "placeholder": "​",
            "style": "IPY_MODEL_d3a22fc5855848a9be0f8e854e767b3d",
            "value": "Generating train split: 100%"
          }
        },
        "43c945e9ad154773bfe9ddd21d84cb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db0d58f0f8d4b5492cd288d52213a3f",
            "max": 52002,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e413d5ebd42a4094b29a0598e29d5981",
            "value": 52002
          }
        },
        "d898aab25e1e4e3db4a788f0a3954a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47797a79533c4413aaa14ed304ada4ed",
            "placeholder": "​",
            "style": "IPY_MODEL_3defe807d7794cf68dd0df58ca0a3bf4",
            "value": " 52002/52002 [00:00&lt;00:00, 318160.05 examples/s]"
          }
        },
        "e850e41203ed44a196c35f977e7fcd2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd315c9799644c13b96fe43e6dea2241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3a22fc5855848a9be0f8e854e767b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3db0d58f0f8d4b5492cd288d52213a3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e413d5ebd42a4094b29a0598e29d5981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47797a79533c4413aaa14ed304ada4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3defe807d7794cf68dd0df58ca0a3bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "cSpBHxxh7_fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "-mAQ9O-p8BQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install editdistance radon nltk"
      ],
      "metadata": {
        "id": "INA0OVIhzf3r",
        "outputId": "b2551788-2509-441e-885e-c12e7e391050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Collecting radon\n",
            "  Downloading radon-6.0.1-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Collecting mando<0.8,>=0.6 (from radon)\n",
            "  Downloading mando-0.7.1-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorama>=0.4.1 (from radon)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mando<0.8,>=0.6->radon) (1.17.0)\n",
            "Downloading radon-6.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading mando-0.7.1-py2.py3-none-any.whl (28 kB)\n",
            "Installing collected packages: mando, colorama, radon\n",
            "Successfully installed colorama-0.4.6 mando-0.7.1 radon-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install \"fschat[model_worker,webui]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYFfrkwV8VqW",
        "outputId": "fd5be13f-cf72-436a-8783-c0544d57ab8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fschat[model_worker,webui]\n",
            "  Downloading fschat-0.2.36-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (3.11.10)\n",
            "Collecting fastapi (from fschat[model_worker,webui])\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (0.28.1)\n",
            "Collecting markdown2[all] (from fschat[model_worker,webui])\n",
            "  Downloading markdown2-2.5.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting nh3 (from fschat[model_worker,webui])\n",
            "  Downloading nh3-0.2.19-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (1.26.4)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (3.0.48)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (2.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (2.32.3)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (13.9.4)\n",
            "Collecting shortuuid (from fschat[model_worker,webui])\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tiktoken (from fschat[model_worker,webui])\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting uvicorn (from fschat[model_worker,webui])\n",
            "  Downloading uvicorn-0.33.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting gradio>=4.10 (from fschat[model_worker,webui])\n",
            "  Downloading gradio-5.9.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: accelerate>=0.21 in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (1.1.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (0.13.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (4.46.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from fschat[model_worker,webui]) (4.25.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat[model_worker,webui]) (0.26.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat[model_worker,webui]) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat[model_worker,webui]) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat[model_worker,webui]) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat[model_worker,webui]) (0.4.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.10->fschat[model_worker,webui]) (3.7.1)\n",
            "Collecting ffmpy (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.10->fschat[model_worker,webui]) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.10->fschat[model_worker,webui]) (3.10.12)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.10->fschat[model_worker,webui]) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.10->fschat[model_worker,webui]) (11.0.0)\n",
            "Collecting pydub (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.10->fschat[model_worker,webui]) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.10->fschat[model_worker,webui]) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio>=4.10->fschat[model_worker,webui]) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio>=4.10->fschat[model_worker,webui]) (14.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio>=4.10->fschat[model_worker,webui])\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->fschat[model_worker,webui]) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->fschat[model_worker,webui]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->fschat[model_worker,webui]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->fschat[model_worker,webui]) (0.14.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat[model_worker,webui]) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat[model_worker,webui]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat[model_worker,webui]) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat[model_worker,webui]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat[model_worker,webui]) (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fschat[model_worker,webui]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fschat[model_worker,webui]) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->fschat[model_worker,webui]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->fschat[model_worker,webui]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->fschat[model_worker,webui]) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->fschat[model_worker,webui]) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->fschat[model_worker,webui]) (4.66.6)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->fschat[model_worker,webui]) (8.1.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat[model_worker,webui]) (1.18.3)\n",
            "Collecting wavedrom (from markdown2[all]->fschat[model_worker,webui])\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting latex2mathml (from markdown2[all]->fschat[model_worker,webui])\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fschat[model_worker,webui]) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fschat[model_worker,webui]) (2.2.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.10->fschat[model_worker,webui]) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.10->fschat[model_worker,webui]) (1.2.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat[model_worker,webui]) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio>=4.10->fschat[model_worker,webui]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio>=4.10->fschat[model_worker,webui]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio>=4.10->fschat[model_worker,webui]) (2024.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.10->fschat[model_worker,webui]) (1.5.4)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat[model_worker,webui])\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat[model_worker,webui]) (1.17.0)\n",
            "Downloading gradio-5.9.0-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.33.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fschat-0.2.36-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.19-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m748.3/748.3 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.2-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wavedrom\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30052 sha256=4f83563be8f3c630b9565e8158733c670d765d583b8597edbcaa4b968e63f16f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "Successfully built wavedrom\n",
            "Installing collected packages: pydub, nh3, uvicorn, tomlkit, svgwrite, shortuuid, semantic-version, ruff, python-multipart, markupsafe, markdown2, latex2mathml, ffmpy, aiofiles, wavedrom, tiktoken, starlette, safehttpx, gradio-client, fastapi, gradio, fschat\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.4.0 fschat-0.2.36 gradio-5.9.0 gradio-client-1.5.2 latex2mathml-3.77.0 markdown2-2.5.2 markupsafe-2.1.5 nh3-0.2.19 pydub-0.25.1 python-multipart-0.0.19 ruff-0.8.3 safehttpx-0.1.6 semantic-version-2.10.0 shortuuid-1.0.13 starlette-0.41.3 svgwrite-1.4.3 tiktoken-0.8.0 tomlkit-0.13.2 uvicorn-0.33.0 wavedrom-2.0.3.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BbmdE7YAMC1",
        "outputId": "f01a372c-aec5-4994-bbf1-3d802789d11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_i1RcOMVrre",
        "outputId": "896f0a79-50c7-4840-8b22-3fb9ca67faf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD-I6BZbYfp0",
        "outputId": "84ca22a0-090f-43ea-9de4-c5633f8053d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=de49b9a14de996d211ef967209751a8b0ac534acdbecb15d50c20e489c0459ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "qhRACvcrDqin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"The attention mask and the pad token id were not set.*\")"
      ],
      "metadata": {
        "id": "JQBuz9K2Dzyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUjMAQ-D5t0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzipping the trained draft model from google drive"
      ],
      "metadata": {
        "id": "fCKBp4hl5uFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cfa445-69e9-4e42-acc4-624e15e013f2",
        "id": "na4NNYTe5uFy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/improved_draft_model.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.printdir()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7eaaaa-2eac-4dea-f1f7-4e7eca9b5086",
        "id": "hbiWDHk35uFy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name                                             Modified             Size\n",
            "improved_draft_model/                          2024-12-13 01:43:10            0\n",
            "improved_draft_model/special_tokens_map.json   2024-12-13 01:43:10          437\n",
            "improved_draft_model/tokenizer_config.json     2024-12-13 01:43:10          948\n",
            "improved_draft_model/tokenizer.json            2024-12-13 01:43:10      3619280\n",
            "improved_draft_model/model.safetensors         2024-12-13 01:43:10    649681952\n",
            "improved_draft_model/config.json               2024-12-13 01:43:08          716\n",
            "improved_draft_model/tokenizer.model           2024-12-13 01:43:10       499723\n",
            "improved_draft_model/generation_config.json    2024-12-13 01:43:08          102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/My Drive/Colab Notebooks/improved_draft_model.zip'\n",
        "\n",
        "# Unzip the file\n",
        "unzip_path = '/content/drive/My Drive/Colab Notebooks/improved_draft_model'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_path)\n",
        "\n",
        "print(\"Unzipping complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b60ab4-5fe6-47f0-cb1a-7fe6e3970159",
        "id": "UYAfEetf5uFy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Llama Model"
      ],
      "metadata": {
        "id": "ogeKlervn8_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import inspect\n",
        "\n",
        "# Load the Draft Model (Improved Draft Model)\n",
        "draft_model_path = \"/content/drive/My Drive/Colab Notebooks/improved_draft_model/improved_draft_model\"  # Unzipped folder path\n",
        "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_path)\n",
        "draft_tokenizer = AutoTokenizer.from_pretrained(draft_model_path)\n",
        "\n",
        "# Load Target Model (LLaMA-2-7B) for both vanilla generation and verification in speculative sampling\n",
        "target_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "target_model = AutoModelForCausalLM.from_pretrained(target_model_name)\n",
        "target_tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
        "\n",
        "if target_tokenizer.pad_token is None:\n",
        "    target_tokenizer.pad_token = target_tokenizer.eos_token\n",
        "if draft_tokenizer.pad_token is None:\n",
        "    draft_tokenizer.pad_token = draft_tokenizer.eos_token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "4975867e17a74a03bde9f485711e095d",
            "2c35fb6bc20b46e0b78fb6cb04906d1f",
            "d65691bc0bb143f39cb840392ffca91c",
            "20192817950a403a85485d7104129094",
            "5679189b041348c99729546934dfffc5",
            "55af656731194514b3ef5e6554971467",
            "2a21d7d8e1734704a431fa4a2a3e27f8",
            "8022b31345d74111a5b98fb7ac3f78d6",
            "2b11a68a601d41b8b1e660a89813c913",
            "428c651b7c0b40bcb5e93969fbe53e41",
            "3217f796f614412389fadfa7de91f60e",
            "8fa82a9c0bf547c19e8d3a330473d405",
            "585788494678407f9e34eeabb8e965a5",
            "7e70d503e29143c7911d02a9a6d1391c",
            "fb158ecadb4b46a48c360f222cdc7154",
            "6f585b84123840b6b8bbeb901aa2981e",
            "4a8edeb878514b589e1c1f62084ca5d8",
            "a85612868c994b6682382ae2b9d044c6",
            "75447139983b4c4d81a57cdf2469dd5b",
            "18d27a9063a1456085ee2654ecfc5cee",
            "11071535c3e04c50be73fd85b904f55d",
            "42f9402268394ff0a33e0c72a884604f",
            "e867bec43b5845d4862fb9a0f101a598",
            "b659e2f6e3fb45aaadb15b025742d2c1",
            "40608406e5d54b989f7dd0a2f3d1e684",
            "acb535c38fc44463aec89185c073a136",
            "57daec5e269145dfa1650f1de0e58c7c",
            "4339e56613d64df9a43761b6b643e624",
            "12da0e702c6f47ea98f1d6a253ad3cbb",
            "bcc498b66e714b84bdfece907a6ae18b",
            "439f44aafbf04f5b9b5dd65eacff0a68",
            "1eb654e853a44e1f97ccafe38ac3f17f",
            "a0fbd811b3b64a85af0ca81a77d8e113",
            "69e73198569746eea2e6927777196a10",
            "88c2fa2ee00a4d049cd1788ffcde40c3",
            "dd3da97022f44449b5474c88257abaf1",
            "b2b0b1fb316f49a4b30248d2a38f6fe6",
            "0d9d38ef924f4dc49775781f133db16c",
            "0493d7cc9a2f4101870bb13bf465eaaf",
            "210eabe13631403b8ea135c247af9cd4",
            "85d27f1db84e49d6b4fc5837328d6085",
            "e38262147c4344d38f8e0a979526ac2c",
            "ead36c4c903c4fcb83d6a43f852b0646",
            "df42e5f45f05443baf063c5f455bfd87",
            "e419e453f4fa4acf845ee63c12c0411b",
            "b24eea17bf1c4beaae7ae1f3c6112342",
            "7789de6fc5bc442c8f15d2f5e64b91d2",
            "ed14d0275a374b2e9c55a72d2662f0a6",
            "35f9202fcd1241b0b21041b7cca74cb3",
            "c6df59d2a0c3487ab74d7e68691e25ea",
            "5385e9f280934b52b01c616e434e9b3c",
            "eca48c7212314753b4c584319b7e2794",
            "152a23cb4d104f9f867fc6026faf3ede",
            "cb84b72907994f2785f7177d8d9017ad",
            "09841d00a40b493a8637cb1eede1ab27",
            "b13af03087604e2d8cfd3c939e742465",
            "dd53d018adf64f02ae024877a1a4cfb6",
            "800a28c15c12429eb95029a6f4f0027b",
            "c267443c192c4ca8af25ff28dd41f3b4",
            "93a3b1e3532d49949b3c2d2514146f65",
            "134480701bb64a95926d94a86e9099c9",
            "873d5c56e9c94c329eb88925802a2245",
            "9b5789303e134920ab00ef5f19c2b3ca",
            "3cbe4077be2e408ab8cc27d79f5afafc",
            "e1fff5761811452e9c5a95bb93ecac95",
            "52245fa9021a4a2ebafcc923df1595e1",
            "c478e7c73b7444ab98315683cb44fecb",
            "81f849257ad443c1961ccaa7e894c09e",
            "61c1c09211d848978c11db7c9f2115c4",
            "a230e6e061084bc2bb0df82dd6d49fda",
            "b7e3ffd72d0b4d65be026fbc3a5657c7",
            "58f049726f83454d88d52a051880651c",
            "a367155e8f484dcd81cc752a82d9b836",
            "98941e82fc3d4a04b86d7929731ca1ec",
            "9ce2ae41bf724879a95e3e35741cf682",
            "6de9d5a8644a41f58db1a4cd80c38599",
            "6e27df0b81864747a03ad861b7c72ec1",
            "1f9a3f7006e54a7b9b03a84cd97a5d48",
            "6126a8be6cd84c2da1b05899b7f581e4",
            "5f0eac648125443f9e8b0bfbae185b4c",
            "91c5358e3ac44a8e81cd0f88726d23d7",
            "ef99672c926d4fb28f7e93c7fbc4e6fe",
            "b84f7bda550e43efb9c5c9ca671f7d70",
            "ddbaff305e794dfa872436968b524df0",
            "2fcd5ed0976848fab1457fe00ecb05f0",
            "276d9952e424408db6cae6bf938382e3",
            "1b2d3dcb50614523b41e0efd16b59201",
            "f9afa5a836994c84a5c185f8745de271",
            "ee9da98701994142b7ddc66e44c2e295",
            "1ced3c1591ff4b53b23d8007df9b386e",
            "1dd2e9bb4a7547f68a759a59b0d569bf",
            "32cf093d103a4164802849ddebd41c68",
            "ab36dfda552a45299c3b9c44a0b5bb29",
            "ac1a9992fe5145af806d9b4987f299d1",
            "daf754d987b44f1bb9b5309af168a4b4",
            "1a57460ebb724e3b9e48177bd52e70d6",
            "4da04151c5454641906172c3403a1162",
            "c03fdcb49baf47938deb827b72199b7b",
            "8761f860c06f4fbfb6df594bdca0a080",
            "b07997d3aeed4a7fa2408c1ebeb9db63",
            "56de422470174dffbf8cad2ab931f001",
            "7d7bbfb29bf14d0ebd52751f54111e1f",
            "e52736e700e346ee8e981cd725eeb843",
            "982c7136599849a2b603e28a3db89523",
            "ffd37ff1156f43bf8ef4c08660a8d04f",
            "7b6c4388db434eb4b59b6f34edd26235",
            "6f3ccb5aeb4b41b19f93865992bfd556",
            "69d9da75f32e4a38b75717068617987b",
            "a57584aab00c4ab28af6882a96e658ba",
            "3b165f1c9b754923ab9b31a9ae675645",
            "efd1af09bf6c40af948d90ca02357c04",
            "a4da4516568647e996fab9eb98594e76",
            "112526326b4a41ec80d79fb4f5bf1e3c",
            "e44e2992673745c6983afa4f38d5d6ee",
            "a1e7cc3539c34083b8f63d145d67ffd9",
            "5ae895295ef64b9da63f5e785f00de4f",
            "164ec6175d8e4fbaab4cf680401d51b2",
            "4e4e25874aed4f8289ce9310ff875546",
            "47b081bcee11420ba39991823a0022e4",
            "bbb9bb8802ea4350b6b6588637dc9747",
            "217706d339c2409e92e9cb3602f8f46b"
          ]
        },
        "id": "wmAY1Ihm0laF",
        "outputId": "980ecd1f-b191-4ef6-a96c-5d7cffac4219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4975867e17a74a03bde9f485711e095d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fa82a9c0bf547c19e8d3a330473d405"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e867bec43b5845d4862fb9a0f101a598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69e73198569746eea2e6927777196a10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e419e453f4fa4acf845ee63c12c0411b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b13af03087604e2d8cfd3c939e742465"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c478e7c73b7444ab98315683cb44fecb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f9a3f7006e54a7b9b03a84cd97a5d48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee9da98701994142b7ddc66e44c2e295"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b07997d3aeed4a7fa2408c1ebeb9db63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efd1af09bf6c40af948d90ca02357c04"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Load ROUGE metric and Sentence-BERT model\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "zavr6pHQ4-Ui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "42dbd081adce424693c4ad1176c26b1e",
            "77fbb932a1504e4884c8d5c99abc8b7f",
            "29ecabceec5047a6a22e3a982f7884e5",
            "2fd9eb896c294ef1a6ec0f1a037b4531",
            "56660c4c0696404b8a620c3d6b4ddd33",
            "831939e5244b4fe69a721b1ceb36a9de",
            "c9981202e9844afa9e836bdeb935e16c",
            "cd4400e748954121b4134877ae7713b4",
            "a113570540624a31ae4848fe557471f1",
            "e3f9eedfae4144239c69092cccac3898",
            "b329164620c441798e446c905b72e4db",
            "aa4485acd6354c919c732b772df11cac",
            "d95bf35c8b974eb589235bb21517ebde",
            "15d97605b48044588f7261225e7172e9",
            "fcd03d9e76854c6ba924dea0d2022587",
            "fc6031c3aa584388bccfe7044cf605e0",
            "ec3f0892c3df4e91a41750ee60a71c61",
            "5a8bba555bb64681be8cbb4390fc4801",
            "af55b1b1bc1543ac869e4a49b34e506d",
            "7f9e31d0a0b141fc8697e9ad3594c0b6",
            "4f75342ee7474d189a0d5d38ae3b8305",
            "5f20d56150614abcaa62769361ac55b1",
            "e90fba342eaf4740aba2f1543bf9819d",
            "8ad29fe283924b5fa81562d01ce2cc4f",
            "b119b221f03e424a836daeba33627eee",
            "48e0624a7f2f4a0a933a895b600dbf81",
            "21e1fb5392e346dd8c6dce4e9e0ffef6",
            "8f6d3b12f57c40138ef2e36a6380b3a6",
            "dd4493244aa04fb5bae7c4a546ea48d0",
            "a296dad9a34e4497b0d13d1243745b58",
            "1be02d48484546bd9a334a14ca58449a",
            "5ab2f4682f144861af923dd850193ed4",
            "4e9659097f2a42e8ba700589dd9cdadd",
            "9fef32b949ea4c32bd81a0d51580bbc6",
            "0f9c76c20d0840908cddd2f910cd2dc7",
            "3f3aebb4306a4cbba56c32a754a8fcc3",
            "3969b496c25b4edcae9b5d0ebde0f91c",
            "60b6b0da3c4140e0b2b5974d5584e42f",
            "cd43b4ae4423463581e95822ab47c4f6",
            "0376cf246d4d4692ac5e1850db436c75",
            "a6ef7b053ed24fb58e0c475e24301462",
            "2427c848c7b845dea2ec263ad22c801d",
            "63fa1462ee714baabc61b1085ac8596e",
            "e71cd88e8e224ce08b99a05978ac0cf9",
            "38b3b0dd603f4be1b5e271f25d44f49b",
            "c30c3557d3974af08e5e6ae15562e817",
            "22cfcafaef65496eab1eef9ab82ca301",
            "02309359b597475eb11c2d732b8e468f",
            "7a102e77bfc54d94b098a9a5a4df7a20",
            "316e9bf4267547079076e002306a612f",
            "3241d6f6d2c4450c9a91d7d2c7287ec7",
            "b20cb36ead384c8ea87387b94378e890",
            "923b8c7bdb984631b3166ef886f67fdf",
            "4f26ca1291fd4d2aa4653cefb3fba27a",
            "5fc45663a7e440ac87809bedd97c5433",
            "d14b977834894816bd5720062f0be83d",
            "a26dc35ae6df4584bc50a37399b35735",
            "310eef4214894e78bc8a088729097c76",
            "721a0e697e1943f689606d25841ed241",
            "15481698b6d44400a767b00d1749d067",
            "6cf2b763a4de4652bf0fc15ec5c567bd",
            "84df9c07222546bf945f8d1293e5ad9d",
            "e5cc399af5184800a4779567b1eb256c",
            "a58a9008d8d54221a00bdd3ebca64308",
            "2dab3350ea8d478090f7447b9f870dae",
            "47ab2f787a2a4ecba929181c27626d6b",
            "114f8636dbf84abc8a21a73e911b9e06",
            "224cfaf140414408b3edbc71af671099",
            "c4e025464b7b4d42b3bd75873c12d96f",
            "e2628d50dee049fdb23e7249c940d392",
            "e0e364699c2249ffbc4760be28c2e9b7",
            "2ec8fe9cc012428c83896697278770a8",
            "36a71204973b4c75b38ad353a3b62955",
            "66a5d956eb5541888b34492a2cc91b71",
            "3e8a841036cd42888c7b0a7bc334233d",
            "c53401dd4ce6479ab66b05839fc22170",
            "6ff116f04f5547178415f24ca3546bcb",
            "caba4de8a5a14d93bdcb10855ca104e4",
            "5975a24fc1a04ff5aa83ed329d8ec572",
            "c0175388c3184b5d9b677b8b1d07ef8e",
            "26d9f51df57a465f88427eb89f10002d",
            "bb50c10d7c9b41d8bd275af57266de8e",
            "fb93985c963b4d08919516e499dba7c8",
            "965bf765b4c04b08a6d41d53c113bdf1",
            "bc2e27c2b7844153b82d7f35b0e87363",
            "2b8381b81366487a9ac3be770416ae75",
            "cfde5161b39d4eaa9247f8e472b63c0d",
            "8ea853a37b5948b4ab5c9cf8965ee00c",
            "6dea1f65ec264848a670c3ec3c2441bb",
            "1099be9264b54963825ee8df8906fb37",
            "e7c6413f3e264573872b24eac2ded35f",
            "4cebfbf3039140589d8b6c68940e7d6a",
            "8330ad32f8324a9dbfaf9cdd61e41edc",
            "895bbdf3ebee49ef80b449eb53d047ae",
            "23474725f5ab449a93c13f45940d2b85",
            "2ff54fd5941646b282e9997add7576fc",
            "a7391d2c144143019561365ed4177407",
            "e9edbcdd6a434c95830c039d7a0cc11e",
            "a7eb2bb9f2df437284a37bfc3d9416fb",
            "68771d16206941988c543348845455cf",
            "2d4266334e3647759780153c3091676a",
            "cf2c1180c63147d7ab40d5fe9573accd",
            "f2c3239b52c044ce838c4a045443c485",
            "a238bee01ea3461fb8d77ac20f1da406",
            "9fb5cb7c03d9445881eecbe4d870d049",
            "d009c81f43264f64938fb747c41acd02",
            "31db28fcc4674f229567488358553142",
            "78d8f9bd55384c66b38b603e9658496a",
            "20d832e73bb74b088774bc7209eb3306",
            "5878388007184c07b06d6c89156cdd0d",
            "eda82ec6a7374cea8aac0fc1fdad40b0",
            "0dec6367cd5b47c68086efe8e97a8338",
            "362b0efe763742059115343e3b5af71e",
            "6ed30834879e4e39b90e31271aec6f05",
            "6bad7728ef2a4bf391f1e3615a299d45",
            "dbb92ac85b13480d90dfde1ee95973f0",
            "f9bc096b3dbf40caa53c6c6cd8b0872d",
            "80a103b4e1aa49f3a26909532e26046c",
            "bade2c1ae0bb498aa700fd80120d5c43",
            "b5b3cb0c984f404999c4499b0c92d688",
            "a418c1e6601d4e078fe91c1d78c58047",
            "84e7ccd35ee14808bda6de851f5177c8",
            "ceead924a9ef4bd89549643881e5959a",
            "7e28b22171d94a5fb186e3a7cc50bf7f",
            "1ed6d606e18d4abab68606c16a8bfe17",
            "5cc6364d9da548d594664e436ceb8f10",
            "04fc77f663194a81b2dfefd7efe2ae12",
            "245742486b2647d08489f86f33709aca",
            "fcd3242b029841d19d851f9bf7ec7f58",
            "a4eb74eff95044cb850e036911e1daf3",
            "54c02a6a3902495cb44456b6cd6d7434",
            "afb59ec28288493f9b0c799d9c54ce26"
          ]
        },
        "outputId": "18f9c8f9-f8f9-4286-c7d9-192a9746f13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42dbd081adce424693c4ad1176c26b1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa4485acd6354c919c732b772df11cac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e90fba342eaf4740aba2f1543bf9819d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fef32b949ea4c32bd81a0d51580bbc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38b3b0dd603f4be1b5e271f25d44f49b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d14b977834894816bd5720062f0be83d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "114f8636dbf84abc8a21a73e911b9e06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caba4de8a5a14d93bdcb10855ca104e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dea1f65ec264848a670c3ec3c2441bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68771d16206941988c543348845455cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eda82ec6a7374cea8aac0fc1fdad40b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84e7ccd35ee14808bda6de851f5177c8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Alpaca Dataset"
      ],
      "metadata": {
        "id": "udxm1Svk2eCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "alpaca_dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
        "print(alpaca_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "52247574206c4ec6bf58ff0711816d65",
            "a2c9d43abbf54470b3ddca9eda200528",
            "f62532f4eb334197aac1894fefa19301",
            "761b65faaa484e1c88576ced33e3521d",
            "4c523d0af93a4cc7ae5c4c7f742f1f9f",
            "73cd906f7b01430ebf23c6d30c9bee0e",
            "6969b7e08fc04c6fa0c2d4608f9b0ed1",
            "37baf2a797514150a6ccffaf01aea16f",
            "d26a4de6d72f4a768ee0067f901183bd",
            "346081f2c3064f7cb407b4a9a96822f6",
            "16ede40e2c6b4b37b6ef68b1cbfe0128",
            "de650b541ecb42e49493d9a520fc103b",
            "445dd2514ec34e0aab64b2c189f95d9f",
            "0292bd7196cd4f2fb2a39244ec26ce1a",
            "3f54425fc8f141ea9abf2e5cb68b0cfc",
            "b6204c6b5a374cfb97de640041555f0e",
            "21459a23abf641ee9f0eabbc0bf151ff",
            "ac27b08413ce4b9da28c244b788de4df",
            "d0c97e29332e4416bb7b07301efbb02d",
            "5832878c00b34ec0b9cc02102f5f036c",
            "70df0c04023d4af68eff2fc33d67a52a",
            "2367f2be51984ee0a6fdb830d6cbead2",
            "4dc425eadcc242b7aa83f1a890069676",
            "093a850bebd9418b9652beaa0209b4e3",
            "43c945e9ad154773bfe9ddd21d84cb69",
            "d898aab25e1e4e3db4a788f0a3954a9a",
            "e850e41203ed44a196c35f977e7fcd2e",
            "cd315c9799644c13b96fe43e6dea2241",
            "d3a22fc5855848a9be0f8e854e767b3d",
            "3db0d58f0f8d4b5492cd288d52213a3f",
            "e413d5ebd42a4094b29a0598e29d5981",
            "47797a79533c4413aaa14ed304ada4ed",
            "3defe807d7794cf68dd0df58ca0a3bf4"
          ]
        },
        "id": "wBOAbrob2hcT",
        "outputId": "ae97f04b-b7e4-410e-9635-99bccb0547df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52247574206c4ec6bf58ff0711816d65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)-00000-of-00001-a09b74b3ef9c3b56.parquet:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de650b541ecb42e49493d9a520fc103b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dc425eadcc242b7aa83f1a890069676"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['instruction', 'input', 'output', 'text'],\n",
            "        num_rows: 52002\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline\n",
        "**max new tokens = 50**\n",
        "\n",
        "**number of data points per run = 500**"
      ],
      "metadata": {
        "id": "a6BOVU_-2TI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0） base base"
      ],
      "metadata": {
        "id": "ih1oRiN86yts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "\n",
        "            # 2. Target model evaluates draft tokens\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1] + self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)\n",
        "\n",
        "            draft_probs = torch.stack(draft_outputs.scores, dim=1)  # Stack all scores\n",
        "            draft_probs = F.softmax(draft_probs, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "            draft_tokens = draft_sequence\n",
        "            p_probs = target_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "            q_probs = draft_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            # Compute acceptance probabilities for all gamma tokens\n",
        "            accept_probs = (p_probs / q_probs).clamp(max=1.0)\n",
        "            random_vals = torch.rand(accept_probs.shape, device=self.device)\n",
        "            accepted = random_vals < accept_probs\n",
        "\n",
        "            # 3. Handle acceptance and rejections\n",
        "            accepted_tokens = draft_tokens[accepted]\n",
        "            total_accepted_tokens += accepted_tokens.shape[0]\n",
        "\n",
        "            if accepted_tokens.numel() > 0:\n",
        "                generated = torch.cat([generated, accepted_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # For rejected tokens, adjust distribution and sample from target model\n",
        "            if not accepted.all():\n",
        "                rejected_indices = (~accepted).nonzero(as_tuple=True)[0]\n",
        "                for idx in rejected_indices:\n",
        "                    adjusted_probs = target_probs[:, idx, :] - draft_probs[:, idx, :]\n",
        "                    adjusted_probs = torch.clamp(adjusted_probs, min=0)\n",
        "                    adjusted_probs /= adjusted_probs.sum(-1, keepdim=True)\n",
        "                    new_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                    generated = torch.cat([generated, new_token], dim=1)\n",
        "\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "            # Print the prompt, reference, and outputs\n",
        "            print(f\"\\nSample {idx + 1}:\")\n",
        "            print(f\"Prompt: {item['full_text']}\")\n",
        "            print(f\"Reference: {reference}\")\n",
        "            print(f\"Vanilla Output: {trad_text}\")\n",
        "            print(f\"Speculative Sampling Output: {spec_text}\")\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=3)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o7UHrXo2V3g",
        "outputId": "2c8d7533-6ed2-4162-f770-83f67f1a3989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample 1:\n",
            "Prompt: Give three tips for staying healthy.\n",
            "Reference: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.\n",
            "Vanilla Output: Give three tips for staying healthy.\n",
            "In order to stay healthy I would recommend the following tips:\n",
            "• Staying active\n",
            "• Eating a well-balanced diet\n",
            "• Having a healthy mental state\n",
            "What do you consider to be the hardest part\n",
            "Speculative Sampling Output: Give three tips for staying healthy.LISTHaveWeCWhatBeWeDonGetAnswer2What  sheSP901664708103Rellying7461in us Matt Sarah R the91937720secondLocal\n",
            "\n",
            "Sample 2:\n",
            "Prompt: What are the three primary colors?\n",
            "Reference: The three primary colors are red, blue, and yellow.\n",
            "Vanilla Output: What are the three primary colors?\n",
            "What are the 4 primary colors of light?\n",
            "What are the 4 primary colors of light used in color vision?\n",
            "What are the 7 primary colors?\n",
            "What is the darkest color?\n",
            "What is the most beautiful color\n",
            "Speculative Sampling Output: What are the three primary colors?vWhereThreeColorColor8AreTheWhyA8 AreO used surr has isPrimary isWhat1 arekColorPrimary mayfor and PWhat are\n",
            " \" Colors theburn di primary putbab Newaeiesiesiesies yuych\n",
            "\n",
            "Sample 3:\n",
            "Prompt: Describe the structure of an atom.\n",
            "Reference: An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
            "Vanilla Output: Describe the structure of an atom. The nucleus of an atom is made of protons and neutrons and has a positive charge. The electrons orbit around the nucleus. The amount of protons, neutrons and electrons determines the number of electrons in an atom. An atom\n",
            "Speculative Sampling Output: Describe the structure of an atom.ListInCompAllWhatIdentConAnTheExpotAnNotPreCompanyForPaDomLPr\n",
            "\n",
            "\n",
            "mo’Himempux55000uxcupp0 Raj9980\n",
            "Mmundmundmund from\n",
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.7073\n",
            "Acceptance_rate: 30.0000\n",
            "Bleu: 0.0000\n",
            "Edit_distance: 203.3333\n",
            "Semantic_similarity: 0.5956\n",
            "Rouge1: 0.1557\n",
            "Rouge2: 0.0723\n",
            "Rougel: 0.1266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1） Temperature = 1, Gamma = 10"
      ],
      "metadata": {
        "id": "RSUJtGFr8ivo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "\n",
        "            # 2. Target model evaluates draft tokens\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1] + self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)\n",
        "\n",
        "            draft_probs = torch.stack(draft_outputs.scores, dim=1)  # Stack all scores\n",
        "            draft_probs = F.softmax(draft_probs, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "            draft_tokens = draft_sequence\n",
        "            p_probs = target_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "            q_probs = draft_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            # Compute acceptance probabilities for all gamma tokens\n",
        "            accept_probs = (p_probs / q_probs).clamp(max=1.0)\n",
        "            random_vals = torch.rand(accept_probs.shape, device=self.device)\n",
        "            accepted = random_vals < accept_probs\n",
        "\n",
        "            # 3. Handle acceptance and rejections\n",
        "            accepted_tokens = draft_tokens[accepted]\n",
        "            total_accepted_tokens += accepted_tokens.shape[0]\n",
        "\n",
        "            if accepted_tokens.numel() > 0:\n",
        "                generated = torch.cat([generated, accepted_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # For rejected tokens, adjust distribution and sample from target model\n",
        "            if not accepted.all():\n",
        "                rejected_indices = (~accepted).nonzero(as_tuple=True)[0]\n",
        "                for idx in rejected_indices:\n",
        "                    adjusted_probs = target_probs[:, idx, :] - draft_probs[:, idx, :]\n",
        "                    adjusted_probs = torch.clamp(adjusted_probs, min=0)\n",
        "                    adjusted_probs /= adjusted_probs.sum(-1, keepdim=True)\n",
        "                    new_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                    generated = torch.cat([generated, new_token], dim=1)\n",
        "\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uocY3Q7JJHZ3",
        "outputId": "e1cede9c-a7ab-4e58-9bff-a38e56f98f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.5353\n",
            "Acceptance_rate: 21.8000\n",
            "Bleu: 0.0202\n",
            "Edit_distance: 287.8920\n",
            "Semantic_similarity: 0.5083\n",
            "Rouge1: 0.1782\n",
            "Rouge2: 0.0816\n",
            "Rougel: 0.1509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Temperature = 1, Gamma = 15"
      ],
      "metadata": {
        "id": "lSNFTpBmyaYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 15\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "\n",
        "            # 2. Target model evaluates draft tokens\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1] + self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)\n",
        "\n",
        "            draft_probs = torch.stack(draft_outputs.scores, dim=1)  # Stack all scores\n",
        "            draft_probs = F.softmax(draft_probs, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "            draft_tokens = draft_sequence\n",
        "            p_probs = target_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "            q_probs = draft_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            # Compute acceptance probabilities for all gamma tokens\n",
        "            accept_probs = (p_probs / q_probs).clamp(max=1.0)\n",
        "            random_vals = torch.rand(accept_probs.shape, device=self.device)\n",
        "            accepted = random_vals < accept_probs\n",
        "\n",
        "            # 3. Handle acceptance and rejections\n",
        "            accepted_tokens = draft_tokens[accepted]\n",
        "            total_accepted_tokens += accepted_tokens.shape[0]\n",
        "\n",
        "            if accepted_tokens.numel() > 0:\n",
        "                generated = torch.cat([generated, accepted_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # For rejected tokens, adjust distribution and sample from target model\n",
        "            if not accepted.all():\n",
        "                rejected_indices = (~accepted).nonzero(as_tuple=True)[0]\n",
        "                for idx in rejected_indices:\n",
        "                    adjusted_probs = target_probs[:, idx, :] - draft_probs[:, idx, :]\n",
        "                    adjusted_probs = torch.clamp(adjusted_probs, min=0)\n",
        "                    adjusted_probs /= adjusted_probs.sum(-1, keepdim=True)\n",
        "                    new_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                    generated = torch.cat([generated, new_token], dim=1)\n",
        "\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbeSQS3dyb8W",
        "outputId": "e243dab4-78f6-4a09-a7da-6277507731ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.3983\n",
            "Acceptance_rate: 21.4767\n",
            "Bleu: 0.0198\n",
            "Edit_distance: 309.2520\n",
            "Semantic_similarity: 0.5005\n",
            "Rouge1: 0.1700\n",
            "Rouge2: 0.0783\n",
            "Rougel: 0.1437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Temperature = 1, Gamma = 5"
      ],
      "metadata": {
        "id": "aiWrmeCa47lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 5\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "\n",
        "            # 2. Target model evaluates draft tokens\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1] + self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)\n",
        "\n",
        "            draft_probs = torch.stack(draft_outputs.scores, dim=1)  # Stack all scores\n",
        "            draft_probs = F.softmax(draft_probs, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "            draft_tokens = draft_sequence\n",
        "            p_probs = target_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "            q_probs = draft_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            # Compute acceptance probabilities for all gamma tokens\n",
        "            accept_probs = (p_probs / q_probs).clamp(max=1.0)\n",
        "            random_vals = torch.rand(accept_probs.shape, device=self.device)\n",
        "            accepted = random_vals < accept_probs\n",
        "\n",
        "            # 3. Handle acceptance and rejections\n",
        "            accepted_tokens = draft_tokens[accepted]\n",
        "            total_accepted_tokens += accepted_tokens.shape[0]\n",
        "\n",
        "            if accepted_tokens.numel() > 0:\n",
        "                generated = torch.cat([generated, accepted_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # For rejected tokens, adjust distribution and sample from target model\n",
        "            if not accepted.all():\n",
        "                rejected_indices = (~accepted).nonzero(as_tuple=True)[0]\n",
        "                for idx in rejected_indices:\n",
        "                    adjusted_probs = target_probs[:, idx, :] - draft_probs[:, idx, :]\n",
        "                    adjusted_probs = torch.clamp(adjusted_probs, min=0)\n",
        "                    adjusted_probs /= adjusted_probs.sum(-1, keepdim=True)\n",
        "                    new_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                    generated = torch.cat([generated, new_token], dim=1)\n",
        "\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTC9Dn8Z48b9",
        "outputId": "60fdbb82-5386-461d-cecd-d4d0bf4a563b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.1683\n",
            "Acceptance_rate: 24.5600\n",
            "Bleu: 0.0198\n",
            "Edit_distance: 285.0560\n",
            "Semantic_similarity: 0.5040\n",
            "Rouge1: 0.1719\n",
            "Rouge2: 0.0753\n",
            "Rougel: 0.1439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Temperature = 0, Gamma = 10"
      ],
      "metadata": {
        "id": "jmVqsWwaB7T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 0\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            if self.temperature > 0:\n",
        "                logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.temperature == 0:\n",
        "                # Greedy decoding for deterministic generation\n",
        "                output = self.target_model.generate(\n",
        "                    context.to(self.device),\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    do_sample=False  # Disable sampling\n",
        "                )\n",
        "            else:\n",
        "                # Stochastic decoding with temperature\n",
        "                output = self.target_model.generate(\n",
        "                    context.to(self.device),\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature  # Use specified temperature\n",
        "                )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature if self.temperature > 0 else 1.0,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "\n",
        "            # 2. Target model evaluates draft tokens\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1] + self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)\n",
        "\n",
        "            draft_probs = torch.stack(draft_outputs.scores, dim=1)  # Stack all scores\n",
        "            draft_probs = F.softmax(draft_probs, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "            draft_tokens = draft_sequence\n",
        "            p_probs = target_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "            q_probs = draft_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            # Compute acceptance probabilities for all gamma tokens\n",
        "            accept_probs = (p_probs / q_probs).clamp(max=1.0)\n",
        "            random_vals = torch.rand(accept_probs.shape, device=self.device)\n",
        "            accepted = random_vals < accept_probs\n",
        "\n",
        "            # 3. Handle acceptance and rejections\n",
        "            accepted_tokens = draft_tokens[accepted]\n",
        "            total_accepted_tokens += accepted_tokens.shape[0]\n",
        "\n",
        "            if accepted_tokens.numel() > 0:\n",
        "                generated = torch.cat([generated, accepted_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # For rejected tokens, adjust distribution and sample from target model\n",
        "            if not accepted.all():\n",
        "                rejected_indices = (~accepted).nonzero(as_tuple=True)[0]\n",
        "                for idx in rejected_indices:\n",
        "                    adjusted_probs = target_probs[:, idx, :] - draft_probs[:, idx, :]\n",
        "                    adjusted_probs = torch.clamp(adjusted_probs, min=0)\n",
        "                    adjusted_probs /= adjusted_probs.sum(-1, keepdim=True)\n",
        "                    new_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                    generated = torch.cat([generated, new_token], dim=1)\n",
        "\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwGkWvf3ThUE",
        "outputId": "5dc25bac-dec0-4016-c83e-09d29655d617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.5505\n",
            "Acceptance_rate: 21.0680\n",
            "Bleu: 0.0202\n",
            "Edit_distance: 288.1720\n",
            "Semantic_similarity: 0.5070\n",
            "Rouge1: 0.1794\n",
            "Rouge2: 0.0810\n",
            "Rougel: 0.1501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Temperature = 0, Gamma = 15"
      ],
      "metadata": {
        "id": "4exg-_43B_Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 15\n",
        "        self.temperature = 0\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            if self.temperature > 0:\n",
        "                logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.temperature == 0:\n",
        "                # Greedy decoding for deterministic generation\n",
        "                output = self.target_model.generate(\n",
        "                    context.to(self.device),\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    do_sample=False  # Disable sampling\n",
        "                )\n",
        "            else:\n",
        "                # Stochastic decoding with temperature\n",
        "                output = self.target_model.generate(\n",
        "                    context.to(self.device),\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature  # Use specified temperature\n",
        "                )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature if self.temperature > 0 else 1.0,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "\n",
        "            # 2. Target model evaluates draft tokens\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1] + self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)\n",
        "\n",
        "            draft_probs = torch.stack(draft_outputs.scores, dim=1)  # Stack all scores\n",
        "            draft_probs = F.softmax(draft_probs, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "            draft_tokens = draft_sequence\n",
        "            p_probs = target_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "            q_probs = draft_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            # Compute acceptance probabilities for all gamma tokens\n",
        "            accept_probs = (p_probs / q_probs).clamp(max=1.0)\n",
        "            random_vals = torch.rand(accept_probs.shape, device=self.device)\n",
        "            accepted = random_vals < accept_probs\n",
        "\n",
        "            # 3. Handle acceptance and rejections\n",
        "            accepted_tokens = draft_tokens[accepted]\n",
        "            total_accepted_tokens += accepted_tokens.shape[0]\n",
        "\n",
        "            if accepted_tokens.numel() > 0:\n",
        "                generated = torch.cat([generated, accepted_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # For rejected tokens, adjust distribution and sample from target model\n",
        "            if not accepted.all():\n",
        "                rejected_indices = (~accepted).nonzero(as_tuple=True)[0]\n",
        "                for idx in rejected_indices:\n",
        "                    adjusted_probs = target_probs[:, idx, :] - draft_probs[:, idx, :]\n",
        "                    adjusted_probs = torch.clamp(adjusted_probs, min=0)\n",
        "                    adjusted_probs /= adjusted_probs.sum(-1, keepdim=True)\n",
        "                    new_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                    generated = torch.cat([generated, new_token], dim=1)\n",
        "\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkbx5bjoagBs",
        "outputId": "f617150b-9405-4639-b9e2-ec54befd52d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.4382\n",
            "Acceptance_rate: 20.6733\n",
            "Bleu: 0.0221\n",
            "Edit_distance: 305.7000\n",
            "Semantic_similarity: 0.5066\n",
            "Rouge1: 0.1770\n",
            "Rouge2: 0.0817\n",
            "Rougel: 0.1490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Temperature = 0, Gamma = 5"
      ],
      "metadata": {
        "id": "4YzANdndCC6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 5\n",
        "        self.temperature = 0\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            if self.temperature > 0:\n",
        "                logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.temperature == 0:\n",
        "                # Greedy decoding for deterministic generation\n",
        "                output = self.target_model.generate(\n",
        "                    context.to(self.device),\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    do_sample=False  # Disable sampling\n",
        "                )\n",
        "            else:\n",
        "                # Stochastic decoding with temperature\n",
        "                output = self.target_model.generate(\n",
        "                    context.to(self.device),\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature  # Use specified temperature\n",
        "                )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature if self.temperature > 0 else 1.0,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "\n",
        "            # 2. Target model evaluates draft tokens\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1] + self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)\n",
        "\n",
        "            draft_probs = torch.stack(draft_outputs.scores, dim=1)  # Stack all scores\n",
        "            draft_probs = F.softmax(draft_probs, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "            draft_tokens = draft_sequence\n",
        "            p_probs = target_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "            q_probs = draft_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            # Compute acceptance probabilities for all gamma tokens\n",
        "            accept_probs = (p_probs / q_probs).clamp(max=1.0)\n",
        "            random_vals = torch.rand(accept_probs.shape, device=self.device)\n",
        "            accepted = random_vals < accept_probs\n",
        "\n",
        "            # 3. Handle acceptance and rejections\n",
        "            accepted_tokens = draft_tokens[accepted]\n",
        "            total_accepted_tokens += accepted_tokens.shape[0]\n",
        "\n",
        "            if accepted_tokens.numel() > 0:\n",
        "                generated = torch.cat([generated, accepted_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # For rejected tokens, adjust distribution and sample from target model\n",
        "            if not accepted.all():\n",
        "                rejected_indices = (~accepted).nonzero(as_tuple=True)[0]\n",
        "                for idx in rejected_indices:\n",
        "                    adjusted_probs = target_probs[:, idx, :] - draft_probs[:, idx, :]\n",
        "                    adjusted_probs = torch.clamp(adjusted_probs, min=0)\n",
        "                    adjusted_probs /= adjusted_probs.sum(-1, keepdim=True)\n",
        "                    new_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                    generated = torch.cat([generated, new_token], dim=1)\n",
        "\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMAw--Qjgl_k",
        "outputId": "6f46f69b-6163-4952-f949-3019eceaef27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.1805\n",
            "Acceptance_rate: 24.3680\n",
            "Bleu: 0.0183\n",
            "Edit_distance: 284.6440\n",
            "Semantic_similarity: 0.5062\n",
            "Rouge1: 0.1738\n",
            "Rouge2: 0.0745\n",
            "Rougel: 0.1452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance Sampling"
      ],
      "metadata": {
        "id": "YX_SiW8h3ozx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0) Base base"
      ],
      "metadata": {
        "id": "9rdZtMNAApMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 1.0\n",
        "        self.top_k_candidates = 50\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                chosen_token = top_k_indices[chosen_idx]\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "            # Print the prompt, reference, and outputs\n",
        "            print(f\"\\nSample {idx + 1}:\")\n",
        "            print(f\"Prompt: {item['full_text']}\")\n",
        "            print(f\"Reference: {reference}\")\n",
        "            print(f\"Vanilla Output: {trad_text}\")\n",
        "            print(f\"Speculative Sampling Output: {spec_text}\")\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=3)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29EmVDKLFc-f",
        "outputId": "0c6bb258-2ca4-4d4d-e980-0dff45d6ff09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample 1:\n",
            "Prompt: Give three tips for staying healthy.\n",
            "Reference: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.\n",
            "Vanilla Output: Give three tips for staying healthy.\n",
            "I have found that exercise, diet and water are the best ways to stay healthy.\n",
            "How long does it take for you to finish a long run?\n",
            "A long run for me is any run over 20km. I\n",
            "Speculative Sampling Output: Give three tips for staying healthy.. must livet go things, the reading out their way,\n",
            "1ostine for of over and the as di of people women10 is2.- the5 to years4s theis of3. to  by for7 with\n",
            "\n",
            "Sample 2:\n",
            "Prompt: What are the three primary colors?\n",
            "Reference: The three primary colors are red, blue, and yellow.\n",
            "Vanilla Output: What are the three primary colors?\n",
            "A. Red, green, and blue.\n",
            "B. Red, yellow, and blue.\n",
            "C. Blue, yellow, and magenta.\n",
            "D. Red, yellow, and blue.\n",
            "What does it mean to say that color\n",
            "Speculative Sampling Output: What are the three primary colors?Where are, complicated most important- color to the primary them red ThisA three p to, primary important blue primary red ( connected An ATheifference color color primary\n",
            " are in is two red ( and blue are Sir of orange are and  pur\n",
            "\n",
            "Sample 3:\n",
            "Prompt: Describe the structure of an atom.\n",
            "Reference: An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
            "Vanilla Output: Describe the structure of an atom. How does it relate to the location of electrons?\n",
            "An atom is the smallest unit of matter. Atoms are composed of subatomic particles, including a nucleus, electrons and neutrons.\n",
            "The nucleus, which contains protons and neut\n",
            "Speculative Sampling Output: Describe the structure of an atom. matters also collection atom is a mixture of:gen and organucymific, of Nabol, H2x H H. stra made elementance highest H, Mx Oeld H, (x M D (H) Cd M2\n",
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 2.2568\n",
            "Acceptance_rate: 74.6667\n",
            "Bleu: 0.0000\n",
            "Edit_distance: 214.0000\n",
            "Semantic_similarity: 0.6127\n",
            "Rouge1: 0.2125\n",
            "Rouge2: 0.0522\n",
            "Rougel: 0.1748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Temperature = 1, Gamma = 10, K = 50"
      ],
      "metadata": {
        "id": "56OLjGD8Aywx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 1.0\n",
        "        self.top_k_candidates = 50\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                chosen_token = top_k_indices[chosen_idx]\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euPCp_drAxmz",
        "outputId": "3f9143bf-66fc-4ad0-859c-10e706bac2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.4858\n",
            "Acceptance_rate: 77.9400\n",
            "Bleu: 0.0176\n",
            "Edit_distance: 274.6740\n",
            "Semantic_similarity: 0.5104\n",
            "Rouge1: 0.1977\n",
            "Rouge2: 0.0691\n",
            "Rougel: 0.1586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Temperature = 1, Gamma = 15, K = 50"
      ],
      "metadata": {
        "id": "HsLC8P3mG79V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k_candidates = 50\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                chosen_token = top_k_indices[chosen_idx]\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOBI3pEvG9SV",
        "outputId": "c318e2b5-c1b6-4425-b629-5ffe7f0fccce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.3680\n",
            "Acceptance_rate: 78.5567\n",
            "Bleu: 0.0166\n",
            "Edit_distance: 288.4620\n",
            "Semantic_similarity: 0.5069\n",
            "Rouge1: 0.1916\n",
            "Rouge2: 0.0638\n",
            "Rougel: 0.1501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Temperature = 1, Gamma = 5, K = 50"
      ],
      "metadata": {
        "id": "7T9QO-0hMzLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 5\n",
        "        self.temperature = 1.0\n",
        "        self.top_k_candidates = 50\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                chosen_token = top_k_indices[chosen_idx]\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmNDlPgbMxK6",
        "outputId": "b8915007-4905-49d9-b3ec-4ea6b30adce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.1526\n",
            "Acceptance_rate: 78.4560\n",
            "Bleu: 0.0177\n",
            "Edit_distance: 272.7520\n",
            "Semantic_similarity: 0.5126\n",
            "Rouge1: 0.1924\n",
            "Rouge2: 0.0693\n",
            "Rougel: 0.1555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Temperature = 0, Gamma = 10, K = 50"
      ],
      "metadata": {
        "id": "AU8bkN60o3sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 0\n",
        "        self.top_k_candidates = 50\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            if self.temperature > 0:\n",
        "                logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=(self.temperature > 0),  # Disable sampling if temperature = 0\n",
        "                temperature=self.temperature if self.temperature > 0 else None  # Avoid invalid temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=(self.temperature > 0),  # Disable sampling if temperature = 0\n",
        "                    temperature=self.temperature if self.temperature > 0 else None,  # Avoid invalid temperature\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                if self.temperature > 0:\n",
        "                    chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                    chosen_token = top_k_indices[chosen_idx]\n",
        "                else:\n",
        "                    chosen_token = torch.argmax(p_dist)  # Deterministic selection when temperature = 0\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        \"\"\"Evaluate with a variety of metrics.\"\"\"\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        rouge = evaluate.load(\"rouge\")\n",
        "        semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QujXQsxFndFl",
        "outputId": "6ff1e219-674f-4b4a-cf5d-7b6484664397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.5936\n",
            "Acceptance_rate: 98.6160\n",
            "Bleu: 0.0244\n",
            "Edit_distance: 279.3060\n",
            "Semantic_similarity: 0.5314\n",
            "Rouge1: 0.2035\n",
            "Rouge2: 0.0853\n",
            "Rougel: 0.1729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Temperature = 0, Gamma = 15, K = 50"
      ],
      "metadata": {
        "id": "86OcLwOVx4M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 15\n",
        "        self.temperature = 0\n",
        "        self.top_k_candidates = 50\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            if self.temperature > 0:\n",
        "                logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=(self.temperature > 0),  # Disable sampling if temperature = 0\n",
        "                temperature=self.temperature if self.temperature > 0 else None  # Avoid invalid temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=(self.temperature > 0),  # Disable sampling if temperature = 0\n",
        "                    temperature=self.temperature if self.temperature > 0 else None,  # Avoid invalid temperature\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                if self.temperature > 0:\n",
        "                    chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                    chosen_token = top_k_indices[chosen_idx]\n",
        "                else:\n",
        "                    chosen_token = torch.argmax(p_dist)  # Deterministic selection when temperature = 0\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        \"\"\"Evaluate with a variety of metrics.\"\"\"\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        rouge = evaluate.load(\"rouge\")\n",
        "        semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT4vjpTwxXh4",
        "outputId": "33a66f53-c2f0-4848-cac7-efc9c85adf93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.4730\n",
            "Acceptance_rate: 99.1300\n",
            "Bleu: 0.0238\n",
            "Edit_distance: 299.2320\n",
            "Semantic_similarity: 0.5312\n",
            "Rouge1: 0.1925\n",
            "Rouge2: 0.0794\n",
            "Rougel: 0.1638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Temperature = 0, Gamma = 5, K = 50"
      ],
      "metadata": {
        "id": "2zrauGio67nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 5\n",
        "        self.temperature = 0\n",
        "        self.top_k_candidates = 50\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            if self.temperature > 0:\n",
        "                logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=(self.temperature > 0),  # Disable sampling if temperature = 0\n",
        "                temperature=self.temperature if self.temperature > 0 else None  # Avoid invalid temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=(self.temperature > 0),  # Disable sampling if temperature = 0\n",
        "                    temperature=self.temperature if self.temperature > 0 else None,  # Avoid invalid temperature\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                if self.temperature > 0:\n",
        "                    chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                    chosen_token = top_k_indices[chosen_idx]\n",
        "                else:\n",
        "                    chosen_token = torch.argmax(p_dist)  # Deterministic selection when temperature = 0\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        \"\"\"Evaluate with a variety of metrics.\"\"\"\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        rouge = evaluate.load(\"rouge\")\n",
        "        semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV1_Ob373Ouv",
        "outputId": "d9061143-9bce-4124-aa5b-76add40544b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.2043\n",
            "Acceptance_rate: 97.5000\n",
            "Bleu: 0.0203\n",
            "Edit_distance: 279.7060\n",
            "Semantic_similarity: 0.5268\n",
            "Rouge1: 0.1960\n",
            "Rouge2: 0.0812\n",
            "Rougel: 0.1667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7）Temperature = 1, Gamma = 10, K= 10"
      ],
      "metadata": {
        "id": "FKZc4rL87QBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 1.0\n",
        "        self.top_k_candidates = 10\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                chosen_token = top_k_indices[chosen_idx]\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl6Hc3Aq7cHk",
        "outputId": "b8f58f2b-74e7-414d-9b66-4d1d9fc68414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.5105\n",
            "Acceptance_rate: 56.2520\n",
            "Bleu: 0.0193\n",
            "Edit_distance: 274.4020\n",
            "Semantic_similarity: 0.5067\n",
            "Rouge1: 0.1919\n",
            "Rouge2: 0.0728\n",
            "Rougel: 0.1574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Temperature = 1, Gamma = 20, K = 20"
      ],
      "metadata": {
        "id": "Vl_wRjF47TN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import editdistance\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class SpeculativeDecodingTester:\n",
        "    def __init__(self, target_tokenizer, target_model, draft_model):\n",
        "        self.tokenizer = target_tokenizer\n",
        "        self.target_model = target_model\n",
        "        self.draft_model = draft_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.target_model.to(self.device)\n",
        "        self.draft_model.to(self.device)\n",
        "\n",
        "        if hasattr(self.target_model, 'config'):\n",
        "            self.target_model.config.use_cache = True\n",
        "        if hasattr(self.draft_model, 'config'):\n",
        "            self.draft_model.config.use_cache = True\n",
        "\n",
        "        # Parameters for speculative sampling\n",
        "        self.max_new_tokens = 50\n",
        "        self.gamma = 10\n",
        "        self.temperature = 1.0\n",
        "        self.top_k_candidates = 20\n",
        "\n",
        "    def get_model_probabilities(self, model, input_ids):\n",
        "        \"\"\"Get probability distribution from model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            logits = logits / self.temperature  # Apply temperature scaling\n",
        "            return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def traditional_generation(self, context):\n",
        "        \"\"\"Traditional token-by-token generation\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.target_model.generate(\n",
        "                context.to(self.device),\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "        return output, end_time - start_time\n",
        "\n",
        "    def speculative_generation(self, context):\n",
        "        start_time = time.time()\n",
        "        generated = context.clone()\n",
        "        total_draft_tokens = 0\n",
        "        total_accepted_tokens = 0\n",
        "\n",
        "        while generated.shape[1] - context.shape[1] < self.max_new_tokens:\n",
        "            # 1. Draft model generates gamma tokens autoregressively\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = self.draft_model.generate(\n",
        "                    generated,\n",
        "                    max_new_tokens=self.gamma,\n",
        "                    do_sample=True,\n",
        "                    temperature=self.temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            draft_sequence = draft_outputs.sequences[:, generated.shape[1]:]\n",
        "            draft_logits = torch.stack(draft_outputs.scores, dim=1)  # [1, gamma, vocab_size]\n",
        "            draft_probs = F.softmax(draft_logits, dim=-1)  # q distribution\n",
        "\n",
        "            # 2. Get target model distributions for these gamma positions in parallel\n",
        "            with torch.no_grad():\n",
        "                target_input = torch.cat([generated, draft_sequence], dim=1)\n",
        "                target_outputs = self.target_model(target_input)\n",
        "                target_logits = target_outputs.logits[:, generated.shape[1]:generated.shape[1]+self.gamma, :]\n",
        "                target_probs = F.softmax(target_logits, dim=-1)  # p distribution\n",
        "\n",
        "            chosen_tokens = []\n",
        "            for i in range(self.gamma):\n",
        "                q_dist = draft_probs[0, i, :]\n",
        "                p_dist = target_probs[0, i, :]\n",
        "\n",
        "                # Get top-K candidates from q\n",
        "                top_k_vals, top_k_indices = torch.topk(q_dist, k=self.top_k_candidates)\n",
        "                p_for_top_k = p_dist[top_k_indices]\n",
        "\n",
        "                # Compute importance weights w = p_for_top_k / top_k_vals\n",
        "                w = p_for_top_k / top_k_vals\n",
        "                w = w / w.sum()  # Normalize weights\n",
        "\n",
        "                # Sample a token from this importance-weighted distribution\n",
        "                chosen_idx = torch.multinomial(w, num_samples=1)\n",
        "                chosen_token = top_k_indices[chosen_idx]\n",
        "\n",
        "                # Compute acceptance probability\n",
        "                chosen_token_prob_p = p_dist[chosen_token].item()\n",
        "                chosen_token_prob_q = q_dist[chosen_token].item()\n",
        "                accept_prob = min(chosen_token_prob_p / chosen_token_prob_q, 1.0)\n",
        "\n",
        "                # Draw a random number to decide acceptance\n",
        "                if torch.rand(1, device=self.device).item() < accept_prob:\n",
        "                    # Accepted the chosen token\n",
        "                    chosen_tokens.append(chosen_token)\n",
        "                    total_accepted_tokens += 1\n",
        "                else:\n",
        "                    # Rejection: We now try the fallback using adjusted distribution: p(x)-q(x)\n",
        "                    adjusted_probs = (p_dist - q_dist).clamp(min=0)\n",
        "                    total_mass = adjusted_probs.sum()\n",
        "                    if total_mass > 0:\n",
        "                        adjusted_probs = adjusted_probs / total_mass\n",
        "                        # Sample from adjusted distribution\n",
        "                        fallback_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "                    else:\n",
        "                        # If adjusted_probs sums to zero, fallback to just picking the top token from p\n",
        "                        fallback_token = torch.argmax(p_dist)\n",
        "                        chosen_tokens.append(fallback_token)\n",
        "\n",
        "            chosen_tokens = torch.tensor(chosen_tokens, device=self.device).unsqueeze(0)\n",
        "            generated = torch.cat([generated, chosen_tokens], dim=1)\n",
        "            total_draft_tokens += self.gamma\n",
        "\n",
        "        acceptance_rate = (total_accepted_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
        "        return generated, time.time() - start_time, acceptance_rate\n",
        "\n",
        "    def prepare_dataset_alpaca(self, dataset, max_samples=100):\n",
        "        \"\"\"Prepare dataset using 'instruction' + 'input' as context and 'output' as target.\"\"\"\n",
        "        processed_data = []\n",
        "        for i, item in enumerate(dataset.select(range(max_samples))):\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output_text = item[\"output\"]\n",
        "\n",
        "            # Combine instruction and input as context\n",
        "            full_context = instruction if not input_text else f\"{instruction}\\n{input_text}\"\n",
        "\n",
        "            # Tokenize context and target, and move them to the correct device\n",
        "            context_tokens = self.tokenizer(full_context, return_tensors=\"pt\", truncation=True, max_length=60).input_ids.squeeze().to(self.device)\n",
        "            target_tokens = self.tokenizer(output_text, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.squeeze().to(self.device)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"context\": context_tokens,\n",
        "                \"target\": target_tokens,\n",
        "                \"full_text\": full_context,\n",
        "                \"reference\": output_text\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def evaluate_with_metrics(self, processed_data):\n",
        "        metrics = {\n",
        "            'speedup': [],\n",
        "            'acceptance_rate': [],\n",
        "            'bleu': [],\n",
        "            'edit_distance': [],\n",
        "            'semantic_similarity': [],\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "\n",
        "        for idx, item in enumerate(processed_data):\n",
        "            context = item[\"context\"]\n",
        "            target = item[\"target\"]\n",
        "            reference = item[\"reference\"]\n",
        "\n",
        "            # Traditional generation\n",
        "            trad_output, trad_time = self.traditional_generation(context.unsqueeze(0))\n",
        "            trad_text = self.tokenizer.decode(trad_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Speculative generation\n",
        "            spec_output, spec_time, spec_accept_rate = self.speculative_generation(context.unsqueeze(0))\n",
        "            spec_text = self.tokenizer.decode(spec_output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute metrics\n",
        "            speedup = trad_time / spec_time\n",
        "            metrics['speedup'].append(speedup)\n",
        "            metrics['acceptance_rate'].append(spec_accept_rate)\n",
        "\n",
        "            # BLEU Score\n",
        "            metrics['bleu'].append(sentence_bleu([reference.split()], spec_text.split()))\n",
        "\n",
        "            # Edit Distance\n",
        "            metrics['edit_distance'].append(editdistance.eval(reference, spec_text))\n",
        "\n",
        "            # ROUGE Scores\n",
        "            rouge_result = rouge.compute(predictions=[spec_text], references=[reference])\n",
        "            metrics['rouge1'].append(rouge_result['rouge1'])\n",
        "            metrics['rouge2'].append(rouge_result['rouge2'])\n",
        "            metrics['rougeL'].append(rouge_result['rougeL'])\n",
        "\n",
        "            # Semantic Similarity\n",
        "            ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
        "            spec_embedding = semantic_model.encode(spec_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(ref_embedding, spec_embedding).item()\n",
        "            metrics['semantic_similarity'].append(similarity)\n",
        "\n",
        "        return {k: np.mean(v) for k, v in metrics.items()}\n",
        "\n",
        "# Initialize tester\n",
        "tester = SpeculativeDecodingTester(target_tokenizer, target_model, draft_model)\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = tester.prepare_dataset_alpaca(alpaca_dataset[\"train\"], max_samples=500)\n",
        "\n",
        "# Evaluate\n",
        "metrics = tester.evaluate_with_metrics(processed_data)\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOtKJFXM7zVi",
        "outputId": "f0675c34-e9a3-4599-a8fd-69f3b684704d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics Summary:\n",
            "Speedup: 1.5162\n",
            "Acceptance_rate: 68.5520\n",
            "Bleu: 0.0178\n",
            "Edit_distance: 273.4400\n",
            "Semantic_similarity: 0.5086\n",
            "Rouge1: 0.1944\n",
            "Rouge2: 0.0700\n",
            "Rougel: 0.1571\n"
          ]
        }
      ]
    }
  ]
}